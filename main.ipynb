{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Huge \\blue{\\textbf{Simple Covariate Shift \\qquad}} \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    classification_report, roc_auc_score, accuracy_score, \n",
    "    f1_score, roc_curve\n",
    ")\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from pygam import s, te, f, LogisticGAM\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import ortho_group\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from src.data_generation import *\n",
    "from src.analysis import ModelEvaluator\n",
    "from src.utils import *\n",
    "from src.plotting import visualize_feature_shifts\n",
    "\n",
    "np.random.seed(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = 'data'\n",
    "\n",
    "# Parameter definition\n",
    "\n",
    "num_samples = 1000\n",
    "num_features = 3\n",
    "\n",
    "# degree of the polinomio for the attribute relationship\n",
    "degree = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random multivariate\n",
    "\n",
    "mean_train = [0.90920214, 0.81962487, 0.88819135]\n",
    "\n",
    "covariance_train = np.array([[0.726318, 0.20240102, 0.52472545],\n",
    "                             [0.20240102, 0.11392557, 0.0264108],\n",
    "                             [0.52472545, 0.0264108, 1.05107627]])\n",
    "\n",
    "# build the features sample\n",
    "sample_train = build_multivariate_sample(num_samples, mean_train, covariance_train)\n",
    "df_train = pd.DataFrame(sample_train, columns=[f'X{i+1}' for i in range(num_features)])\n",
    "\n",
    "# build target variable y\n",
    "# random coefficients (otherwise remove coef from build_poly_target and will be randomly generated)\n",
    "coef = [-0.8061577012389105, -0.3621987584904036, -0.16057091147074054, 0.4803476403769713, -0.10624889645240687, \n",
    "        0.3182084398201366, 0.6789895126695962, -0.791324832566177, 0.531479159887424, 0.49115959567000167]\n",
    "\n",
    "y_train, coef_train = build_poly_target(sample_train, degree, coef)\n",
    "df_train['Y'] = y_train\n",
    "\n",
    "# check for balance\n",
    "df_train['Y'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Sets: Shifted Distribution Mixtures\n",
    "\n",
    "To be as general as possible, we consider statistical mixtures and study the presumed progressive degradation in performance for increasingly pure mixtures towards the test distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shifted random multivariate\n",
    "mean_shift = attributes_quantile(df_train, 0.05)\n",
    "\n",
    "covariance_shift = [[ 0.16309729,  0.19325742, -0.12621892],\n",
    "                    [ 0.19325742,  0.25197638, -0.13972381],\n",
    "                    [-0.12621892, -0.13972381,  0.19160666]]\n",
    "\n",
    "# Initialize an empty dictionary to store the dataframes\n",
    "df_dict = {}\n",
    "\n",
    "# Iterate over mix_prob values\n",
    "for mix_prob in [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]:\n",
    "    # Generate mixture sample\n",
    "    sample_mix = build_mixture_sample(num_samples, mean_train, covariance_train, mean_shift, covariance_shift, mix_prob=mix_prob)\n",
    "\n",
    "    # Create a DataFrame for the features\n",
    "    df_mix = pd.DataFrame(sample_mix, columns=[f'X{i+1}' for i in range(num_features)])\n",
    "\n",
    "    # Build the target variable y\n",
    "    y_mix, coef_mix = build_poly_target(sample_mix, degree, coefficients=coef_train)\n",
    "    df_mix['Y'] = y_mix\n",
    "\n",
    "    # Store the DataFrame in the dictionary\n",
    "    df_dict[mix_prob] = df_mix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remark: the 0.0 is a sample from the distribution that generated the training set. Since `build_mixture_sample` function do the dample each time, the 0.0 sample can be used as test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Data to Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a folder\n",
    "folder_name = os.path.join('data')\n",
    "os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "for mix_prob, df in df_dict.items():\n",
    "    df.to_csv(os.path.join(folder_name, f'mix_{mix_prob}.csv'), index=False)\n",
    "file_name = 'Parameters.txt'\n",
    "file_path = os.path.join(folder_name, file_name)\n",
    "df_train.to_csv(os.path.join(folder_name, 'train.csv'), index=False)\n",
    "\n",
    "with open(file_path, 'w') as f:\n",
    "  f.write('Polinomial coefficients\\n')\n",
    "  f.write(f'{coef_train}\\n')\n",
    "  f.write('Mean train\\n')\n",
    "  f.write(f'{mean_train}\\n')\n",
    "  f.write('Covariance train\\n')\n",
    "  f.write(f'{covariance_train}\\n')\n",
    "  f.write('Mean shift\\n')\n",
    "  f.write(f'{mean_shift}\\n')\n",
    "  f.write('Covariance shift\\n')\n",
    "  f.write(f'{covariance_shift}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 2. Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " For higher dimensional data (n > 2), we can either:\n",
    "\n",
    " - Visualize a pairwise scatter matrix (e.g., `sns.pairplot`) for a subset of features.\n",
    "\n",
    " - Or just visualize a specified pair of features for a quick glimpse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_feature_shifts(df_dict=df_dict, features_to_plot= ['X1', 'X2', 'X3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Models Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train data\n",
    "\n",
    "X_train = df_train.drop('Y', axis=1)\n",
    "y_train = df_train['Y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgam_params = {\n",
    "    \"terms\": s(0) + s(1) + s(2) + te(0, 1) + te(0, 2) + te(1, 2),\n",
    "    \"max_iter\": 100\n",
    "}\n",
    "\n",
    "X_train_np = X_train.values  # Convert to NumPy array\n",
    "y_train_np = y_train.values  # Convert to NumPy array\n",
    "\n",
    "lgam_model = LogisticGAM(**lgam_params).gridsearch(X_train_np, y_train_np, lam=np.logspace(-3, 3, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgam_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'min_samples_leaf': [1, 5, 10, 15, 20]\n",
    "}\n",
    "\n",
    "# Initialize the Decision Tree model\n",
    "dtc = DecisionTreeClassifier()\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=dtc, param_grid=param_grid, cv=5, scoring='roc_auc')\n",
    "\n",
    "# Perform the grid search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and the best model\n",
    "best_params = grid_search.best_params_\n",
    "best_dct = grid_search.best_estimator_\n",
    "\n",
    "print(f\"Best parameters found: {best_params}\")\n",
    "print(f\"Best model: {best_dct}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Parameters\n",
    "\n",
    "rfc_params = {\n",
    "    \"n_estimators\": 100,\n",
    "    \"max_depth\": 4,\n",
    "    \"min_samples_leaf\": 13\n",
    "}\n",
    "\n",
    "rfc_model = RandomForestClassifier(**rfc_params)\n",
    "rfc_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the parameter grid for Random Forest\n",
    "# param_grid_rf = {\n",
    "#     'n_estimators': [50, 100, 150],\n",
    "#     'max_depth': [None, 10, 20, 30],\n",
    "#     'min_samples_split': [2, 5, 10],\n",
    "#     'min_samples_leaf': [1, 2, 4]\n",
    "# }\n",
    "\n",
    "# # Initialize the Random Forest model\n",
    "# rf = RandomForestClassifier()\n",
    "\n",
    "# # Initialize GridSearchCV\n",
    "# grid_search_rf = GridSearchCV(estimator=rf, param_grid=param_grid_rf, cv=5, scoring='accuracy')\n",
    "\n",
    "# # Perform the grid search\n",
    "# grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "# # Get the best parameters and the best model\n",
    "# best_params_rf = grid_search_rf.best_params_\n",
    "# best_rf_model = grid_search_rf.best_estimator_\n",
    "\n",
    "# print(f\"Best parameters found: {best_params_rf}\")\n",
    "# print(f\"Best model: {best_rf_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting Parameters\n",
    "gbc_params = {\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"max_depth\": 4,\n",
    "    \"max_features\": 'log2',\n",
    "    \"min_samples_leaf\": 13,\n",
    "    \"n_estimators\": 100,\n",
    "    \"subsample\": 0.7\n",
    "}\n",
    "\n",
    "gbc_model = GradientBoostingClassifier(**gbc_params)\n",
    "gbc_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# # Define the parameter grid for Gradient Boosting\n",
    "# param_grid_gbc = {\n",
    "#     'n_estimators': [50, 100, 150],\n",
    "#     'learning_rate': [0.01, 0.05, 0.1],\n",
    "#     'max_depth': [3, 4, 5],\n",
    "#     'subsample': [0.7, 0.8, 0.9],\n",
    "#     'max_features': ['auto', 'sqrt', 'log2']\n",
    "# }\n",
    "\n",
    "# # Initialize the Gradient Boosting model\n",
    "# gbc = GradientBoostingClassifier()\n",
    "\n",
    "# # Initialize GridSearchCV\n",
    "# grid_search_gbc = GridSearchCV(estimator=gbc, param_grid=param_grid_gbc, cv=5, scoring='accuracy')\n",
    "\n",
    "# # Perform the grid search\n",
    "# grid_search_gbc.fit(X_train, y_train)\n",
    "\n",
    "# # Get the best parameters and the best model\n",
    "# best_params_gbc = grid_search_gbc.best_params_\n",
    "# best_gbc_model = grid_search_gbc.best_estimator_\n",
    "\n",
    "# print(f\"Best parameters found: {best_params_gbc}\")\n",
    "# print(f\"Best model: {best_gbc_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extreme Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "#XGBoost Parameters\n",
    "xgb_params = {\n",
    "    \"learning_rate\":0.025,\n",
    "    \"max_depth\":5,\n",
    "    \"n_estimators\":100,\n",
    "    \"subsample\":0.7\n",
    "}\n",
    "\n",
    "xgb_model = xgb.XGBClassifier().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 4. Model Evaluation After the Shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models to evaluate\n",
    "\n",
    "models = {\n",
    "    \"LogisticGAM\" : lgam_model,\n",
    "    \"DecisionTreeClassifier\" : best_dct,\n",
    "    \"GradientBoostingClassifier\" : gbc_model,\n",
    "    \"RandomForestClassifier\" : rfc_model,\n",
    "    \"XGBoost\" : xgb_model\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df_dict is a dictionary with keys from 0.1 to 1.0\n",
    "test_datasets = [(key, df.drop('Y', axis=1), df['Y']) for key, df in df_dict.items() if 0.0 <= key <= 1.0]\n",
    "\n",
    "evaluator = ModelEvaluator(models, test_datasets)\n",
    "evaluator.evaluate_models(show_metrics=True)\n",
    "evaluator.plot_roc_curves()\n",
    "evaluator.plot_roc_curves_per_dataset()\n",
    "evaluator.plot_auc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 5. Mechanistic-Interpretability-Guided Robust Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple, Union\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from src.robust_training.mechanistic import MechanisticTrainer\n",
    "try:\n",
    "    from pygam import LogisticGAM\n",
    "    PYGAM_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PYGAM_AVAILABLE = False\n",
    "\n",
    "\n",
    "def run_mechanistic_robust_training_and_eval(\n",
    "    folder: str = \"dat\",\n",
    "    target: str = 'Y',\n",
    "    n_rounds: int = 2,\n",
    "    model_type: str = 'gbc',  # Options: 'gbc', 'tree', 'gam'\n",
    "    base_shift_factor: float = 100,\n",
    "    fraction_to_shift: float = 0.9,\n",
    "    final_train_size: Optional[int] = None,\n",
    "    random_state: int = 42,\n",
    "    noise_scale: float = 0.001,\n",
    "    n_grad_steps: int = 1,\n",
    "    top_k: int = 3,\n",
    "    eps = 0.2, \n",
    "    ball = False\n",
    ") -> Tuple[BaseEstimator, BaseEstimator]:\n",
    "    \"\"\"\n",
    "    Trains both a baseline model and a robust model using MechanisticTrainer,\n",
    "    then evaluates both models on all shifted test files in the specified folder.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    folder : str\n",
    "        Directory containing 'train.csv' for training and 'mix_<n>.csv' for testing.\n",
    "    target : str\n",
    "        Name of the target variable in the datasets.\n",
    "    n_rounds : int\n",
    "        Number of augmentation rounds for MechanisticTrainer.\n",
    "    model_type : str\n",
    "        Type of model to use for robust training. Options: 'gbc', 'tree', 'gam'.\n",
    "    base_shift_factor : float\n",
    "        Magnitude by which to shift selected features during augmentation.\n",
    "    fraction_to_shift : float\n",
    "        Fraction of the dataset to select for augmentation each round.\n",
    "    final_train_size : int or None\n",
    "        If specified, downsample the final training set to this size.\n",
    "    random_state : int\n",
    "        Seed for reproducibility.\n",
    "    noise_scale : float\n",
    "        Standard deviation of Gaussian noise added to augmented samples.\n",
    "    n_grad_steps : int\n",
    "        Number of gradient-based steps per sample during augmentation.\n",
    "    top_k : int\n",
    "        Number of top features (by gradient magnitude) to shift per sample.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    baseline_model : BaseEstimator\n",
    "        The baseline model trained on the original data.\n",
    "    robust_model : BaseEstimator\n",
    "        The robustly trained model using MechanisticTrainer.\n",
    "    \"\"\"\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    # 1) Load original training data from \"mix_0.0.csv\"\n",
    "    # ----------------------------------------------------------------------\n",
    "    train_file = os.path.join(folder, \"train.csv\")\n",
    "    if not os.path.exists(train_file):\n",
    "        raise FileNotFoundError(f\"Training file '{train_file}' not found in folder '{folder}'.\")\n",
    "\n",
    "    df_orig = pd.read_csv(train_file)\n",
    "    if target not in df_orig.columns:\n",
    "        raise ValueError(f\"Target column '{target}' not found in '{train_file}'.\")\n",
    "\n",
    "    X_train = df_orig.drop(columns=[target])\n",
    "    y_train = df_orig[target]\n",
    "\n",
    "    print(f\"Loaded training data from '{train_file}' with shape = {X_train.shape}\")\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    # 2) Train Baseline Model\n",
    "    #    Ensure baseline uses the same model_type for fair comparison.\n",
    "    # ----------------------------------------------------------------------\n",
    "    print(\"\\n=== Training Baseline Model ===\")\n",
    "    if model_type == 'gbc':\n",
    "        baseline_model = GradientBoostingClassifier(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=4,                        # default 3 ma fa pena, a 10 ha migliore AUC ma comunque minore f1, a 4/5 è fair\n",
    "            random_state=random_state\n",
    "        )\n",
    "    elif model_type == 'tree':\n",
    "        baseline_model = DecisionTreeClassifier(\n",
    "            max_depth=10,                       # default 5 ma fa pena, a 10 ha peggiore AUC e comunque minore f1 ma è fair\n",
    "            random_state=random_state\n",
    "        )\n",
    "    elif model_type == 'gam':\n",
    "        if not PYGAM_AVAILABLE:\n",
    "            raise ImportError(\"pyGAM is not installed. Install it via `pip install pygam` or choose another model type.\")\n",
    "        baseline_model = LogisticGAM( verbose=False)\n",
    "        \n",
    "    elif model_type == 'rfc':\n",
    "        baseline_model = RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=10,                   # default 5 ma fa pena, a 10 ha peggiore AUC e comunque minore f1 ma è fair\n",
    "            random_state=random_state\n",
    "        )\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model_type '{model_type}'. Choose from ['gbc', 'tree', 'gam'].\")\n",
    "\n",
    "    baseline_model.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"Baseline model trained.\")\n",
    "    # Check class distribution in shifted data\n",
    "    df_shifted = pd.read_csv('data/mix_1.0.csv')\n",
    "    print(\"Class distribution in shifted data:\")\n",
    "    print(df_shifted['Y'].value_counts(normalize=True))\n",
    "\n",
    "    # Check model predictions\n",
    "    y_pred = baseline_model.predict(df_shifted.drop('Y', axis=1))\n",
    "    print(\"\\nPrediction distribution:\")\n",
    "    print(pd.Series(y_pred).value_counts(normalize=True))\n",
    "    \n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    # 3) Train Mechanistic-Interpretability-Guided Robust Model\n",
    "    # ----------------------------------------------------------------------\n",
    "    print(\"\\n=== Training Mechanistic-Interpretability-Guided Robust Model ===\")\n",
    "    trainer = MechanisticTrainer(\n",
    "        model_type=model_type,         # 'gbc', 'tree', 'gam'\n",
    "        base_shift_factor=base_shift_factor,\n",
    "        n_rounds=n_rounds,\n",
    "        subset_size_fraction=fraction_to_shift,\n",
    "        n_grad_steps=n_grad_steps,\n",
    "        top_k=top_k,\n",
    "        random_state=random_state,\n",
    "        noise_scale=noise_scale,\n",
    "       \n",
    "        val_fraction=0.1,               # Fraction for validation split\n",
    "        eps = eps,\n",
    "        ball = ball\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    # Fit the robust model\n",
    "    trainer.fit(X_train, y_train)\n",
    "    robust_model = trainer.model\n",
    "    print(\"Robust model trained.\")\n",
    "\n",
    "    # If final_train_size is specified, downsample & refit\n",
    "    if final_train_size is not None and final_train_size < len(trainer.X_final):\n",
    "        rng = np.random.RandomState(random_state)\n",
    "        idx_down = rng.choice(len(trainer.X_final), size=final_train_size, replace=False)\n",
    "        X_down = trainer.X_final.iloc[idx_down].reset_index(drop=True)\n",
    "        y_down = trainer.y_final.iloc[idx_down].reset_index(drop=True)\n",
    "        robust_model.fit(X_down, y_down)\n",
    "        print(f\"Final training set downsampled to {final_train_size} samples.\")\n",
    "        # Check class distribution in shifted data\n",
    "    df_shifted = pd.read_csv('data/mix_1.0.csv')\n",
    "    print(\"Class distribution in shifted data:\")\n",
    "    print(df_shifted['Y'].value_counts(normalize=True))\n",
    "\n",
    "    # Check model predictions\n",
    "    y_pred = robust_model.predict(df_shifted.drop('Y', axis=1))\n",
    "    print(\"\\nPrediction distribution:\")\n",
    "    print(pd.Series(y_pred).value_counts(normalize=True))\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    # 4) Evaluate on all shifted test files: \"mix_<n>.csv\"\n",
    "    # ----------------------------------------------------------------------\n",
    "    test_files = [\n",
    "        f for f in os.listdir(folder)\n",
    "        if f.startswith(\"mix_\") and f.endswith(\".csv\") \n",
    "    ]\n",
    "\n",
    "    if not test_files:\n",
    "        print(f\"\\nNo shifted test files found in '{folder}' for evaluation.\")\n",
    "        return baseline_model, robust_model\n",
    "\n",
    "    print(\"\\n=== Evaluation on Shifted Test Files ===\")\n",
    "    for test_file in sorted(test_files):\n",
    "        test_path = os.path.join(folder, test_file)\n",
    "        df_test = pd.read_csv(test_path)\n",
    "        if target not in df_test.columns:\n",
    "            print(f\"Skipping '{test_file}': missing target '{target}'.\")\n",
    "            continue\n",
    "\n",
    "        X_test = df_test.drop(columns=[target])\n",
    "        y_test = df_test[target]\n",
    "\n",
    "        # Evaluate Baseline Model\n",
    "        y_pred_b = baseline_model.predict(X_test)\n",
    "        if hasattr(baseline_model, \"predict_proba\"):\n",
    "            y_proba_b = baseline_model.predict_proba(X_test)[:, 1]\n",
    "            try:\n",
    "                auc_b = roc_auc_score(y_test, y_proba_b)\n",
    "            except ValueError:\n",
    "                auc_b = \"N/A (only one class present)\"\n",
    "        else:\n",
    "            y_proba_b = None\n",
    "            auc_b = \"N/A\"\n",
    "\n",
    "        acc_b = accuracy_score(y_test, y_pred_b)\n",
    "        f1_b = f1_score(y_test, y_pred_b) \n",
    "\n",
    "        # Evaluate Robust Model\n",
    "        y_pred_r = robust_model.predict(X_test)\n",
    "        if hasattr(robust_model, \"predict_proba\"):\n",
    "            y_proba_r = robust_model.predict_proba(X_test)[:, 1]\n",
    "            try:\n",
    "                auc_r = roc_auc_score(y_test, y_proba_r)\n",
    "            except ValueError:\n",
    "                auc_r = \"N/A (only one class present)\"\n",
    "        else:\n",
    "            y_proba_r = None\n",
    "            auc_r = \"N/A\"\n",
    "\n",
    "        acc_r = accuracy_score(y_test, y_pred_r)\n",
    "        f1_r = f1_score(y_test, y_pred_r) \n",
    "        print(f\"\\nTest File: {test_file}\")\n",
    "        print(f\"  Baseline Model => Accuracy: {acc_b:.3f}, F1 Score: {f1_b:.3f}, AUC: {auc_b}\")\n",
    "        print(f\"  Robust Model   => Accuracy: {acc_r:.3f}, F1 Score: {f1_r:.3f}, AUC: {auc_r}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        print(f\"\\nTest File: {test_file}: delta AUC r-b (hope > 0): {auc_r - auc_b}\")\n",
    "\n",
    "    return baseline_model, robust_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_FOLDER = 'data'\n",
    "# TARGET_COLUMN = 'Y'\n",
    "\n",
    "# # Run the robust training and evaluation\n",
    "# baseline_model, robust_model = run_mechanistic_robust_training_and_eval(\n",
    "#     folder=DATA_FOLDER,\n",
    "#     target=TARGET_COLUMN,\n",
    "#     n_rounds=1,\n",
    "#     model_type='rfc',          # Options: 'gbc', 'tree', 'gam', 'rfc'\n",
    "#     base_shift_factor=3.5,    \n",
    "#     fraction_to_shift=0.9,\n",
    "#     final_train_size=None,     # Keep the original size\n",
    "#     random_state=42,\n",
    "#     noise_scale=0.01,\n",
    "#     n_grad_steps=1,\n",
    "#     top_k=3, \n",
    "#     eps=0.2,\n",
    "#     ball=True\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_FOLDER = 'data'\n",
    "# TARGET_COLUMN = 'Y'\n",
    "\n",
    "# for eps in np.arange(0.1, 1.0, 0.1):\n",
    "#     print(f\"###############################Running for epsilon = {eps}\")\n",
    "#     # Run the robust training and evaluation\n",
    "#     baseline_model, robust_model = run_mechanistic_robust_training_and_eval(\n",
    "#         folder=DATA_FOLDER,\n",
    "#         target=TARGET_COLUMN,\n",
    "#         n_rounds=1,\n",
    "#         model_type='rfc',          # Options: 'gbc', 'tree', 'gam', 'rfc'\n",
    "#         base_shift_factor=100,    \n",
    "#         fraction_to_shift=0.9,\n",
    "#         final_train_size=1000,     # Keep the original size\n",
    "#         random_state=42,\n",
    "#         noise_scale=0.0001,\n",
    "#         n_grad_steps=5,\n",
    "#         top_k=3, \n",
    "#         eps=eps\n",
    "#     )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
