{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Huge \\blue{\\textbf{Distribution shift \\qquad}} \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\text{Analysis of different models over shifted datasets}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\text{Imports \\& settings}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    classification_report, roc_auc_score, accuracy_score, \n",
    "    f1_score, roc_curve\n",
    ")\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from pygam import s, te, f, LogisticGAM\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import ortho_group\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from src.data_generation import *\n",
    "from src.analysis import compare_adversarial_training, ModelEvaluator\n",
    "from src.utils import *\n",
    "from src.plotting import visualize_feature_shifts\n",
    "\n",
    "np.random.seed(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = 'data'\n",
    "\n",
    "# Parameter definition\n",
    "\n",
    "num_samples = 1000\n",
    "num_features = 3\n",
    "\n",
    "# degree of the polinomio for the attribute relationship\n",
    "degree = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random multivariate\n",
    "\n",
    "mean_train = [0.90920214, 0.81962487, 0.88819135]\n",
    "\n",
    "covariance_train = np.array([[0.726318, 0.20240102, 0.52472545],\n",
    "                             [0.20240102, 0.11392557, 0.0264108],\n",
    "                             [0.52472545, 0.0264108, 1.05107627]])\n",
    "\n",
    "# build the features sample\n",
    "sample_train = build_multivariate_sample(num_samples, mean_train, covariance_train)\n",
    "df_train = pd.DataFrame(sample_train, columns=[f'X{i+1}' for i in range(num_features)])\n",
    "\n",
    "# build target variable y\n",
    "# random coefficients (otherwise remove coef from build_poly_target and will be randomly generated)\n",
    "coef = [-0.8061577012389105, -0.3621987584904036, -0.16057091147074054, 0.4803476403769713, -0.10624889645240687, \n",
    "        0.3182084398201366, 0.6789895126695962, -0.791324832566177, 0.531479159887424, 0.49115959567000167]\n",
    "\n",
    "y_train, coef_train = build_poly_target(sample_train, degree, coef)\n",
    "df_train['Y'] = y_train\n",
    "\n",
    "# check for balance\n",
    "df_train['Y'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Sets: Shifted Distribution Mixtures\n",
    "\n",
    "To be as general as possible, we consider statistical mixtures and study the presumed progressive degradation in performance for increasingly pure mixtures towards the test distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shifted random multivariate\n",
    "mean_shift = attributes_quantile(df_train, 0.05)\n",
    "\n",
    "covariance_shift = [[ 0.16309729,  0.19325742, -0.12621892],\n",
    "                    [ 0.19325742,  0.25197638, -0.13972381],\n",
    "                    [-0.12621892, -0.13972381,  0.19160666]]\n",
    "\n",
    "# Initialize an empty dictionary to store the dataframes\n",
    "df_dict = {}\n",
    "\n",
    "# Iterate over mix_prob values\n",
    "for mix_prob in [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]:\n",
    "    # Generate mixture sample\n",
    "    sample_mix = build_mixture_sample(num_samples, mean_train, covariance_train, mean_shift, covariance_shift, mix_prob=mix_prob)\n",
    "\n",
    "    # Create a DataFrame for the features\n",
    "    df_mix = pd.DataFrame(sample_mix, columns=[f'X{i+1}' for i in range(num_features)])\n",
    "\n",
    "    # Build the target variable y\n",
    "    y_mix, coef_mix = build_poly_target(sample_mix, degree, coefficients=coef_train)\n",
    "    df_mix['Y'] = y_mix\n",
    "\n",
    "    # Store the DataFrame in the dictionary\n",
    "    df_dict[mix_prob] = df_mix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remark: the 0.0 is a sample from the distribution that generated the training set. Since `build_mixture_sample` function do the dample each time, the 0.0 sample can be used as test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Data to Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a folder\n",
    "folder_name = os.path.join('data')\n",
    "os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "for mix_prob, df in df_dict.items():\n",
    "    df.to_csv(os.path.join(folder_name, f'mix_{mix_prob}.csv'), index=False)\n",
    "file_name = 'Parameters.txt'\n",
    "file_path = os.path.join(folder_name, file_name)\n",
    "\n",
    "with open(file_path, 'w') as f:\n",
    "  f.write('Polinomial coefficients\\n')\n",
    "  f.write(f'{coef_train}\\n')\n",
    "  f.write('Mean train\\n')\n",
    "  f.write(f'{mean_train}\\n')\n",
    "  f.write('Covariance train\\n')\n",
    "  f.write(f'{covariance_train}\\n')\n",
    "  f.write('Mean shift\\n')\n",
    "  f.write(f'{mean_shift}\\n')\n",
    "  f.write('Covariance shift\\n')\n",
    "  f.write(f'{covariance_shift}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 2. Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " For higher dimensional data (n > 2), we can either:\n",
    "\n",
    " - Visualize a pairwise scatter matrix (e.g., `sns.pairplot`) for a subset of features.\n",
    "\n",
    " - Or just visualize a specified pair of features for a quick glimpse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize_feature_shifts(df_dict=df_dict, features_to_plot= ['X1', 'X2', 'X3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Models Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train data\n",
    "\n",
    "X_train = df_train.drop('Y', axis=1)\n",
    "y_train = df_train['Y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgam_params = {\n",
    "    \"terms\": s(0) + s(1) + s(2) + te(0, 1) + te(0, 2) + te(1, 2),\n",
    "    \"max_iter\": 100\n",
    "}\n",
    "\n",
    "lgam_model = LogisticGAM(**lgam_params).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgam_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Parameters\n",
    "\n",
    "dtc_params = {\n",
    "    \"max_depth\": 4,\n",
    "    \"min_samples_leaf\": 13\n",
    "}\n",
    "\n",
    "dtc_model = DecisionTreeClassifier(**dtc_params)\n",
    "dtc_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting Parameters\n",
    "gbc_params = {\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"max_depth\": 4,\n",
    "    \"max_features\": 'log2',\n",
    "    \"min_samples_leaf\": 13,\n",
    "    \"n_estimators\": 100,\n",
    "    \"subsample\": 0.7\n",
    "}\n",
    "\n",
    "gbc_model = GradientBoostingClassifier(**gbc_params)\n",
    "gbc_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extreme Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost Parameters\n",
    "# xgb_params = {\n",
    "#     \"learning_rate\":0.025,\n",
    "#     \"max_depth\":5,\n",
    "#     \"n_estimators\":100,\n",
    "#     \"subsample\":0.7\n",
    "# }\n",
    "\n",
    "# xgb_model = xgb.XGBClassifier().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 4. Model Evaluation After the Distribution Shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models to evaluate\n",
    "\n",
    "models = {\n",
    "    \"LogisticGAM\" : lgam_model,\n",
    "    \"DecisionTreeClassifier\" : dtc_model,\n",
    "    \"GradientBoostingClassifier\" : gbc_model,\n",
    "    # \"XGBoost\" : xgb_model\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df_dict is a dictionary with keys from 0.1 to 1.0\n",
    "test_datasets = [(key, df.drop('Y', axis=1), df['Y']) for key, df in df_dict.items() if 0.0 <= key <= 1.0]\n",
    "\n",
    "evaluator = ModelEvaluator(models, test_datasets)\n",
    "evaluator.evaluate_models(show_metrics=True)\n",
    "evaluator.plot_roc_curves()\n",
    "evaluator.plot_roc_curves_per_dataset()\n",
    "evaluator.plot_accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 5. Adversarial Training for GradientBoosting (General n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Compare Normal vs. Adversarial Training on Shifted Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, info in models.items():\n",
    "  compare_adversarial_training(\n",
    "    DATA_FOLDER, \n",
    "    model_class=info[\"class\"],\n",
    "    model_params=info[\"params\"],\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 6. Mechanistic-Interpretability-Guided Robust Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mech_interp_boosting(\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    target = 'Y',\n",
    "    base_model=None,\n",
    "    base_shift_factor: float = 0.1,\n",
    "    n_rounds: int = 2,\n",
    "    fraction_to_shift: float = 0.3,\n",
    "    min_improvement: float = 0.001,\n",
    "    noise_scale: float = 0.01,\n",
    "    n_features_to_shift: int = 1,\n",
    "    final_train_size: int = None,\n",
    "    use_weighted_fit: bool = False,\n",
    "    random_state: int = 42\n",
    "):\n",
    "    \"\"\"\n",
    "    Improved Mechanistic-Interpretability-Guided Robust Boosting.\n",
    "    \n",
    "    Changes:\n",
    "      1) Per-round, only shift a fraction of samples (fraction_to_shift).\n",
    "      2) Weighted training (optional): shift data can have a higher sample_weight, \n",
    "         so the model doesn't need an even bigger dataset but still focuses on them.\n",
    "      3) Repeated smaller shift_factor, combined with early stopping on a validation set.\n",
    "      4) Final downsampling to ensure fair #samples vs. baseline.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train, y_train : pd.DataFrame, pd.Series\n",
    "        Original data\n",
    "    base_model : fitted or None\n",
    "        If None, a new GradientBoostingClassifier is created.\n",
    "    base_shift_factor : float\n",
    "        Initial magnitude of shift for the selected features.\n",
    "    n_rounds : int\n",
    "        Max number of augmentation rounds.\n",
    "    fraction_to_shift : float\n",
    "        Fraction of samples to shift each round.\n",
    "    min_improvement : float\n",
    "        Minimum improvement in validation for continuing rounds.\n",
    "    noise_scale : float\n",
    "        Std dev of random noise added to each shifted sample.\n",
    "    n_features_to_shift : int\n",
    "        # of top features to shift.\n",
    "    final_train_size : int or None\n",
    "        If not None, downsample the final augmented set to this size.\n",
    "    use_weighted_fit : bool\n",
    "        If True, we apply sample weights that are higher for newly shifted samples.\n",
    "    random_state : int\n",
    "        Reproducibility seed.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model : GradientBoostingClassifier\n",
    "        The final robust model after all rounds.\n",
    "    X_final, y_final : final training set used to fit (with or without weighting).\n",
    "    \"\"\"\n",
    "\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    \n",
    "    if base_model is None:\n",
    "        base_model = GradientBoostingClassifier(\n",
    "            learning_rate=0.05,\n",
    "            max_depth=4,\n",
    "            max_features='log2',\n",
    "            min_samples_leaf=13,\n",
    "            n_estimators=100,\n",
    "            subsample=0.7,\n",
    "            random_state=random_state\n",
    "        )\n",
    "\n",
    "    # Train/Val split\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "        X_train, y_train, test_size=0.2, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # This is the growing augmented set\n",
    "    X_aug = X_tr.copy()\n",
    "    y_aug = y_tr.copy()\n",
    "    # (optional) Keep track of sample weights\n",
    "    sample_weights = np.ones(len(X_aug))  # default weight=1\n",
    "\n",
    "    best_val_score = 0.0\n",
    "    rounds_without_improvement = 0\n",
    "    \n",
    "    for round_idx in range(n_rounds):\n",
    "        # Fit the model with or without weighting\n",
    "        if use_weighted_fit:\n",
    "            base_model.fit(X_aug, y_aug, sample_weight=sample_weights)\n",
    "        else:\n",
    "            base_model.fit(X_aug, y_aug)\n",
    "        \n",
    "        val_score = base_model.score(X_val, y_val)\n",
    "        print(f\"[Round {round_idx+1}] Validation Score = {val_score:.3f}\")\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_score > best_val_score + min_improvement:\n",
    "            best_val_score = val_score\n",
    "            rounds_without_improvement = 0\n",
    "        else:\n",
    "            rounds_without_improvement += 1\n",
    "            if rounds_without_improvement >= 3:\n",
    "                print(f\"Early stopping at round {round_idx+1}\")\n",
    "                break\n",
    "\n",
    "        # SHIFT SAMPLES:\n",
    "        # Only shift fraction_to_shift portion of the data\n",
    "        n_current = len(X_aug)\n",
    "        subset_size = int(fraction_to_shift * n_current)\n",
    "        idx_to_shift = rng.choice(n_current, size=subset_size, replace=False)\n",
    "        \n",
    "        shift_list = []\n",
    "        label_list = []\n",
    "        weight_list = []\n",
    "\n",
    "        current_shift_factor = base_shift_factor * (0.9 ** round_idx)\n",
    "        \n",
    "        #print(\"ciao\")\n",
    "        \n",
    "        for i in idx_to_shift:\n",
    "            \n",
    "            x_i = X_aug.iloc[i].values.copy()\n",
    "            label_i = y_aug.iloc[i]\n",
    "            \n",
    "            # Compute approximate gradient-based feature importance\n",
    "            # Probability of correct label\n",
    "            df_x_i = pd.DataFrame(x_i.reshape(1, -1), columns=X_tr.columns)\n",
    "            prob_original = base_model.predict_proba(df_x_i)[0][label_i]\n",
    "\n",
    "            feature_scores = []\n",
    "            for feat_idx in range(len(x_i)):\n",
    "                x_pos = x_i.copy()\n",
    "                x_neg = x_i.copy()\n",
    "                x_pos[feat_idx] += 1e-3\n",
    "                x_neg[feat_idx] -= 1e-3\n",
    "                df_x_pos = pd.DataFrame(x_pos.reshape(1, -1), columns=X_tr.columns)\n",
    "                df_x_neg = pd.DataFrame(x_neg.reshape(1, -1), columns=X_tr.columns)\n",
    "                prob_pos = base_model.predict_proba(df_x_pos)[0][label_i]\n",
    "                prob_neg = base_model.predict_proba(df_x_neg)[0][label_i]\n",
    "                grad_est = (prob_pos - prob_neg) / (2e-3)\n",
    "                feature_scores.append(abs(grad_est))\n",
    "            \n",
    "            top_feats = np.argsort(feature_scores)[-n_features_to_shift:]\n",
    "            \n",
    "            shift_dir = np.zeros_like(x_i)\n",
    "            for feat_idx in top_feats:\n",
    "                # Recompute sign(grad) to see which direction lowers prob\n",
    "                x_pos[feat_idx] = x_i[feat_idx] + 1e-3\n",
    "                x_neg[feat_idx] = x_i[feat_idx] - 1e-3\n",
    "                df_x_pos = pd.DataFrame(x_pos.reshape(1, -1), columns=X_tr.columns)\n",
    "                df_x_neg = pd.DataFrame(x_neg.reshape(1, -1), columns=X_tr.columns)\n",
    "                prob_pos = base_model.predict_proba(df_x_pos)[0][label_i]\n",
    "                prob_neg = base_model.predict_proba(df_x_neg)[0][label_i]\n",
    "                actual_grad = (prob_pos - prob_neg) / (2e-3)\n",
    "                direction_sign = -1 if actual_grad > 0 else 1\n",
    "                shift_dir[feat_idx] = direction_sign\n",
    "            noise = rng.normal(0, noise_scale, size=len(x_i))\n",
    "            x_shifted = x_i + shift_dir * current_shift_factor + noise\n",
    "            \n",
    "            shift_list.append(x_shifted)\n",
    "            label_list.append(label_i)\n",
    "            if use_weighted_fit:\n",
    "                # e.g., give a slightly higher weight, so model focuses on these new shifted points\n",
    "                weight_list.append(2.0)  # shift points weight=2.0\n",
    "            else:\n",
    "                weight_list.append(1.0)\n",
    "\n",
    "        # Augment\n",
    "        df_shifted = pd.DataFrame(shift_list, columns=X_tr.columns)\n",
    "        s_labels = pd.Series(label_list, name=target)\n",
    "        \n",
    "        X_aug = pd.concat([X_aug, df_shifted], ignore_index=True)\n",
    "        y_aug = pd.concat([y_aug, s_labels], ignore_index=True)\n",
    "        if use_weighted_fit:\n",
    "            sample_weights = np.concatenate([sample_weights, np.array(weight_list)])\n",
    "        else:\n",
    "            # Keep same weighting for original samples\n",
    "            sample_weights = np.ones(len(X_aug))\n",
    "\n",
    "        print(f\"  => Added {len(df_shifted)} new shifted samples; total = {len(X_aug)}\")\n",
    "\n",
    "    # Final training set: combine with X_val\n",
    "    X_final = pd.concat([X_aug, X_val], ignore_index=True)\n",
    "    y_final = pd.concat([y_aug, y_val], ignore_index=True)\n",
    "\n",
    "    final_weights = None\n",
    "    if use_weighted_fit:\n",
    "        # For simplicity, assume new samples from X_val have weight=1\n",
    "        w_val = np.ones(len(X_val))\n",
    "        final_weights = np.concatenate([sample_weights, w_val])\n",
    "    \n",
    "    # Downsample to final_train_size\n",
    "    if final_train_size is not None and final_train_size < len(X_final):\n",
    "        indices = rng.choice(len(X_final), size=final_train_size, replace=False)\n",
    "        X_final = X_final.iloc[indices].reset_index(drop=True)\n",
    "        y_final = y_final.iloc[indices].reset_index(drop=True)\n",
    "        if final_weights is not None:\n",
    "            final_weights = final_weights[indices]\n",
    "\n",
    "    # Final fit\n",
    "    if use_weighted_fit:\n",
    "        base_model.fit(X_final, y_final, sample_weight=final_weights)\n",
    "    else:\n",
    "        base_model.fit(X_final, y_final)\n",
    "\n",
    "    return base_model, X_final, y_final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Demo of Mechanistic Boosting on Shifted Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_mechanistic_robust_boosting(folder: str = \"dat\", target = 'Y', n_rounds: int = 1):\n",
    "    \"\"\"\n",
    "    Demonstrates the Mechanistic-Interpretability-Guided Robust Boosting \n",
    "    approach on 'mix_0.0.csv' and evaluates it on the shifted CSV files.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    folder : str, optional\n",
    "        The folder containing the training dataset 'mix_0.0.csv' and some shifted 'mix_<n>.csv' files (n > 0.0).\n",
    "    n_rounds : int, optional\n",
    "        How many augmentation+retrain rounds to run.\n",
    "    \"\"\"\n",
    "    # 1) Load training data\n",
    "    df_orig = pd.read_csv(os.path.join(folder, \"mix_0.0.csv\"))\n",
    "    X_train = df_orig.drop(target, axis=1)\n",
    "    y_train = df_orig[target]\n",
    "    \n",
    "    # 2) Baseline GBC for comparison\n",
    "    gbc_baseline = GradientBoostingClassifier(\n",
    "        learning_rate=0.05,\n",
    "        max_depth=4,\n",
    "        max_features='log2',\n",
    "        min_samples_leaf=13,\n",
    "        n_estimators=100,\n",
    "        subsample=0.7\n",
    "    )\n",
    "    gbc_baseline.fit(X_train, y_train)\n",
    "    \n",
    "    # 3) Mechanistic interpretability-based boosting\n",
    "    gbc_mi, X_aug, y_aug = mech_interp_boosting(\n",
    "        X_train, \n",
    "        y_train,\n",
    "        target = target,\n",
    "        base_shift_factor=0.2,\n",
    "        n_rounds=n_rounds,\n",
    "        min_improvement=0.001,\n",
    "        noise_scale=0.01,\n",
    "        n_features_to_shift= 4\n",
    "    )\n",
    "    \n",
    "    test_files = [f for f in os.listdir(folder) if f.startswith(\"mix_\")]\n",
    "    print(f\"\\n=== Evaluate Baseline GBC vs. MI-Guided GBC (n_rounds={n_rounds}) on Shifted Data ===\\n\")\n",
    "    \n",
    "    for test_file in test_files:\n",
    "        df_test = pd.read_csv(os.path.join(folder, test_file))\n",
    "        X_test = df_test[X_train.columns]\n",
    "        y_test = df_test[target]\n",
    "        \n",
    "        y_pred_b = gbc_baseline.predict(X_test)\n",
    "        y_proba_b = gbc_baseline.predict_proba(X_test)[:, 1]\n",
    "        acc_b = accuracy_score(y_test, y_pred_b)\n",
    "        f1_b = f1_score(y_test, y_pred_b)\n",
    "        auc_b = roc_auc_score(y_test, y_proba_b)\n",
    "        \n",
    "        \n",
    "        y_pred_m = gbc_mi.predict(X_test)\n",
    "        y_proba_m = gbc_mi.predict_proba(X_test)[:, 1]\n",
    "        acc_m = accuracy_score(y_test, y_pred_m)\n",
    "        f1_m = f1_score(y_test, y_pred_m)\n",
    "        auc_m = roc_auc_score(y_test, y_proba_m)\n",
    "        \n",
    "        print(f\"Shifted file: {test_file}\")\n",
    "        print(f\"  Baseline GBC => Acc: {acc_b:.3f}, F1: {f1_b:.3f}, AUC: {auc_b:.3f}\")\n",
    "        print(f\"  MI-Guided GBC => Acc: {acc_m:.3f}, F1: {f1_m:.3f}, AUC: {auc_m:.3f}\")\n",
    "        print(\"---------------------------------------------------\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo_mechanistic_robust_boosting(DATA_FOLDER, n_rounds=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_baseline_gb(\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    final_train_size: int = None,\n",
    "    random_state: int = 42\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a standard GradientBoosting on exactly 'final_train_size' points \n",
    "    (if provided). If final_train_size < len(X_train), downsample first.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    \n",
    "    if final_train_size is not None and final_train_size < len(X_train):\n",
    "        indices = rng.choice(len(X_train), size=final_train_size, replace=False)\n",
    "        X_train = X_train.iloc[indices].reset_index(drop=True)\n",
    "        y_train = y_train.iloc[indices].reset_index(drop=True)\n",
    "    \n",
    "    model = GradientBoostingClassifier(\n",
    "        learning_rate=0.05,\n",
    "        max_depth=4,\n",
    "        max_features='log2',\n",
    "        min_samples_leaf=13,\n",
    "        n_estimators=100,\n",
    "        subsample=0.7,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 7. Compare All Models: Original, Adversarial, Mechanistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models_performance(folder: str = \"data\", target = 'Y'):\n",
    "    \"\"\"\n",
    "    Compare performance between:\n",
    "      - Original Gradient Boosting\n",
    "      - Adversarially Trained Gradient Boosting\n",
    "      - Mechanistically-Interpreted (MI) Model\n",
    "      \n",
    "    Plots side-by-side bar charts of Accuracy, F1, AUC for each shifted dataset.\n",
    "    \"\"\"\n",
    "    df_orig = pd.read_csv(os.path.join(folder, \"mix_0.0.csv\"))\n",
    "    X_train = df_orig.drop(target, axis=1)\n",
    "    y_train = df_orig[target]\n",
    "\n",
    "    final_size = len(X_train)\n",
    "    gbc_original = train_baseline_gb(X_train, y_train, final_train_size=final_size)\n",
    "    \n",
    "    gbc_adversarial, X_adv, y_adv = adversarial_training(\n",
    "        X_train, y_train,\n",
    "        epsilon=0.1,\n",
    "        max_rounds=3,\n",
    "        fraction_to_perturb=0.5,\n",
    "        final_train_size=final_size\n",
    "    )\n",
    "    \n",
    "    gbc_mech, X_mi, y_mi = mech_interp_boosting(\n",
    "        X_train, y_train,\n",
    "        base_shift_factor=0.1,\n",
    "        n_rounds=3,\n",
    "        fraction_to_shift=0.7,\n",
    "        use_weighted_fit=True,\n",
    "        final_train_size=final_size\n",
    "    )\n",
    "\n",
    "    # Evaluate these three models on all shifted CSVs\n",
    "    test_files = [f for f in os.listdir(folder) if f.startswith(\"mix_\")]\n",
    "    test_files = sorted([f for f in os.listdir(folder) if f.startswith(\"mix_\")], \n",
    "               key=lambda x: int(x.split('_')[1].split('.')[0]))\n",
    "    results = []\n",
    "    model_dict = {\n",
    "        \"OriginalGB\": gbc_original,\n",
    "        \"AdversarialGB\": gbc_adversarial,\n",
    "        \"MechInterpGB\": gbc_mech\n",
    "    }\n",
    "\n",
    "    for model_name, model_obj in model_dict.items():\n",
    "        for test_file in sorted(test_files):\n",
    "            df_test = pd.read_csv(os.path.join(folder, sorted(test_files)[test_files.index(test_file)]))\n",
    "            X_test = df_test[X_train.columns]\n",
    "            y_test = df_test[target]\n",
    "\n",
    "            y_pred = model_obj.predict(X_test)\n",
    "            y_pred_proba = model_obj.predict_proba(X_test)[:, 1]\n",
    "\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            f1_ = f1_score(y_test, y_pred)\n",
    "            auc_ = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "            results.append({\n",
    "                \"Model\": model_name,\n",
    "                \"ShiftedFile\": test_file,\n",
    "                \"Accuracy\": acc,\n",
    "                \"F1\": f1_,\n",
    "                \"AUC\": auc_\n",
    "            })\n",
    "\n",
    "    df_results = pd.DataFrame(results)\n",
    "    metrics_to_plot = [\"Accuracy\", \"F1\", \"AUC\"]\n",
    "    \n",
    "    # Updated plotting code with fixed legends\n",
    "    fig1, axes1 = plt.subplots(3, 1, figsize=(10, 15))\n",
    "    \n",
    "    for idx, metric in enumerate(metrics_to_plot):\n",
    "        sns.barplot(\n",
    "            data=df_results, \n",
    "            x=\"ShiftedFile\", \n",
    "            y=metric, \n",
    "            hue=\"Model\",\n",
    "            ax=axes1[idx]\n",
    "        )\n",
    "        axes1[idx].set_title(f\"{metric} Comparison\")\n",
    "        #axes1[idx].set_xticklabels(\n",
    "        #    axes1[idx].get_xticklabels(), \n",
    "        #    rotation=45, \n",
    "        #    ha='right'\n",
    "        #)\n",
    "        #axes1[idx].set_ylim(0, 1.05)\n",
    "        axes1[idx].legend(title=\"Models\")\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    fig2, axes2 = plt.subplots(3, 1, figsize=(10, 15))\n",
    "    \n",
    "    for idx, metric in enumerate(metrics_to_plot):\n",
    "        pivot_data = df_results.pivot(\n",
    "            index='ShiftedFile', \n",
    "            columns='Model', \n",
    "            values=metric\n",
    "        )\n",
    "        \n",
    "        rel_improvement = 100 * (\n",
    "            pivot_data - pivot_data['OriginalGB'].values.reshape(-1,1)\n",
    "        ) / pivot_data['OriginalGB'].values.reshape(-1,1)\n",
    "        \n",
    "        # Explicitly set labels for legend\n",
    "        rel_improvement_plot = rel_improvement[['AdversarialGB', 'MechInterpGB']].reset_index()\n",
    "        rel_improvement_plot = pd.melt(\n",
    "            rel_improvement_plot, \n",
    "            id_vars=['ShiftedFile'],\n",
    "            var_name='Model',\n",
    "            value_name='Improvement'\n",
    "        )\n",
    "        \n",
    "        sns.barplot(\n",
    "            data=rel_improvement_plot,\n",
    "            x='ShiftedFile',\n",
    "            y='Improvement',\n",
    "            hue='Model',\n",
    "            ax=axes2[idx]\n",
    "        )\n",
    "        \n",
    "        axes2[idx].set_title(f'Relative {metric} Improvement (%)')\n",
    "        axes2[idx].axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "        #axes2[idx].set_xticklabels(\n",
    "        #    axes2[idx].get_xticklabels(), \n",
    "        #    rotation=45,\n",
    "        #    ha='right'\n",
    "        #)\n",
    "        axes2[idx].set_ylabel('Improvement %')\n",
    "        axes2[idx].legend(title=\"Models\")\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare_models_performance(DATA_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ONE CELL TO RUN THEM ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Generate data for n_features\n",
    "#create_shifted_datasets(n_features=4, output_folder=\"data\", base_n_samples=1000, cov_scale=3.5, mean_shift_scale=2.5)\n",
    "\n",
    "# 2) Visualize shifts\n",
    "#visualize_feature_shifts(DATA_FOLDER, features_to_plot=['X1', 'X2', 'X3'])\n",
    "\n",
    "# 3) Evaluate baseline models\n",
    "#evaluate_models_on_shifts(DATA_FOLDER)\n",
    "\n",
    "# 4) Compare adv vs normal\n",
    "#compare_adversarial_training(DATA_FOLDER)\n",
    "\n",
    "# 5) Mechanistic robust boosting\n",
    "#demo_mechanistic_robust_boosting(DATA_FOLDER, n_rounds=2)\n",
    "\n",
    "# 6) Compare all\n",
    "#compare_models_performance(DATA_FOLDER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_mechinterp_hparams(\n",
    "    folder: str = \"data\",\n",
    "    base_shift_candidates = [0.05, 0.1, 0.2],\n",
    "    fraction_candidates = [0.3, 0.5, 0.7],\n",
    "    target = 'Y',\n",
    "    n_rounds: int = 3,\n",
    "    final_train_size: int = None,\n",
    "    random_state: int = 42\n",
    "):\n",
    "    \"\"\"\n",
    "    1. Load the original dataset from `folder`.\n",
    "    2. Train baseline GB + Adversarial GB for reference.\n",
    "    3. For each (base_shift_factor, fraction_to_shift) in the grid:\n",
    "       - Train a MechInterp model.\n",
    "       - Evaluate on the same shifted CSVs.\n",
    "    4. Compare performance + Return a table of all results + Identify best combo.\n",
    "    Parameters\n",
    "    ----------\n",
    "    folder : str\n",
    "        Folder containing mix_*.csv.\n",
    "    base_shift_candidates : list\n",
    "        Values to try for `base_shift_factor`.\n",
    "    fraction_candidates : list\n",
    "        Values to try for `fraction_to_shift`.\n",
    "    n_rounds : int\n",
    "        Rounds of MechInterp training.\n",
    "    final_train_size : int or None\n",
    "        If not None, downsample final training set to keep dataset size fair.\n",
    "    random_state : int\n",
    "        For reproducibility.\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A DataFrame summarizing all runs (Model, base_shift, fraction, ShiftFile, Accuracy, F1, AUC).\n",
    "    (float, float)\n",
    "        The best (base_shift_factor, fraction_to_shift) combo (based on chosen metric).\n",
    "    \"\"\"\n",
    "    # 1) Load original => train data\n",
    "    df_orig = pd.read_csv(os.path.join(folder, \"mix_0.0.csv\"))\n",
    "    X_train = df_orig.drop(target, axis=1)\n",
    "    y_train = df_orig[target]\n",
    "    if final_train_size is None:\n",
    "        final_train_size = len(X_train)  # default to full training size\n",
    "    # 2) Train baseline\n",
    "    gbc_baseline = train_baseline_gb(\n",
    "        X_train, y_train, \n",
    "        final_train_size=final_train_size, \n",
    "        random_state=random_state\n",
    "    )\n",
    "    # 2b) Train adversarial\n",
    "    gbc_adv, X_adv, y_adv = adversarial_training_gbc(\n",
    "        X_train, y_train,\n",
    "        epsilon=0.1,\n",
    "        max_rounds=3,\n",
    "        fraction_to_perturb=0.5,\n",
    "        final_train_size=final_train_size,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    # We'll evaluate on each shifted file\n",
    "    test_files = [f for f in os.listdir(folder) if f.startswith(\"shifted_\")]\n",
    "  \n",
    "    # Collect all results in a list of dicts\n",
    "    all_results = []\n",
    "  \n",
    "    # Evaluate Baseline + Adversarial once here\n",
    "    model_dict = {\n",
    "        \"OriginalGB\": gbc_baseline,\n",
    "        \"AdversarialGB\": gbc_adv\n",
    "    }\n",
    "  \n",
    "    for model_name, model_obj in model_dict.items():\n",
    "        for test_file in test_files:\n",
    "            df_test = pd.read_csv(os.path.join(folder, test_file))\n",
    "            X_test = df_test[X_train.columns]\n",
    "            y_test = df_test[target]\n",
    "          \n",
    "            y_pred = model_obj.predict(X_test)\n",
    "            y_proba = model_obj.predict_proba(X_test)[:, 1]\n",
    "          \n",
    "            all_results.append({\n",
    "                \"Model\": model_name,\n",
    "                \"BaseShiftFactor\": None,\n",
    "                \"FractionShift\": None,\n",
    "                \"ShiftedFile\": test_file,\n",
    "                \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "                \"F1\": f1_score(y_test, y_pred),\n",
    "                \"AUC\": roc_auc_score(y_test, y_proba)\n",
    "            })\n",
    "    # 3) For each combo of base_shift_factor, fraction_to_shift => train MechInterp\n",
    "    for base_shift in base_shift_candidates:\n",
    "        for fraction_shift in fraction_candidates:\n",
    "            model_name_combo = f\"MechInterp_s{base_shift}_f{fraction_shift}\"\n",
    "          \n",
    "            # Train MechInterp\n",
    "            gbc_mech, X_mech, y_mech = mech_interp_boosting(\n",
    "                X_train, y_train,\n",
    "                base_model=None,\n",
    "                base_shift_factor=base_shift,\n",
    "                n_rounds=n_rounds,\n",
    "                fraction_to_shift=fraction_shift,\n",
    "                min_improvement=0.001,\n",
    "                noise_scale=0.01,\n",
    "                n_features_to_shift=2,  # or your choice\n",
    "                final_train_size=final_train_size,\n",
    "                use_weighted_fit=True,\n",
    "                random_state=random_state\n",
    "            )\n",
    "          \n",
    "            # Evaluate on each shifted file\n",
    "            for test_file in sorted(test_files):\n",
    "                df_test = pd.read_csv(os.path.join(folder, test_file))\n",
    "                X_test = df_test[X_train.columns]\n",
    "                y_test = df_test[target]\n",
    "              \n",
    "                y_pred = gbc_mech.predict(X_test)\n",
    "                y_proba = gbc_mech.predict_proba(X_test)[:, 1]\n",
    "              \n",
    "                all_results.append({\n",
    "                    \"Model\": model_name_combo,\n",
    "                    \"BaseShiftFactor\": base_shift,\n",
    "                    \"FractionShift\": fraction_shift,\n",
    "                    \"ShiftedFile\": test_file,\n",
    "                    \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "                    \"F1\": f1_score(y_test, y_pred),\n",
    "                    \"AUC\": roc_auc_score(y_test, y_proba)\n",
    "                })\n",
    "    df_all = pd.DataFrame(all_results)\n",
    "    # 4) Identify best combo:\n",
    "    # Let's pick \"best\" as highest average AUC across all shifted files\n",
    "    # (You can change to F1 or average of all metrics.)\n",
    "    # Filter only rows with Model starting in \"MechInterp_\"\n",
    "    df_mech = df_all[df_all[\"Model\"].str.startswith(\"MechInterp_s\")]\n",
    "  \n",
    "    # Group by (BaseShiftFactor, FractionShift), average the AUC\n",
    "    grouped = df_mech.groupby([\"BaseShiftFactor\", \"FractionShift\"])[\"AUC\"].mean().reset_index()\n",
    "    best_row = grouped.loc[grouped[\"AUC\"].idxmax()]  # row with highest mean AUC\n",
    "    best_base_shift = best_row[\"BaseShiftFactor\"]\n",
    "    best_fraction = best_row[\"FractionShift\"]\n",
    "    best_auc = best_row[\"AUC\"]\n",
    "  \n",
    "    print(\"\\n======== MechInterp Hyperparam Tuning Results ========\")\n",
    "    print(df_mech.groupby([\"BaseShiftFactor\", \"FractionShift\"])[[\"Accuracy\",\"F1\",\"AUC\"]].mean())\n",
    "    print(\"======================================================\\n\")\n",
    "    print(f\"Best Combination (by avg AUC): base_shift={best_base_shift}, fraction={best_fraction}, AUC={best_auc:.3f}\")\n",
    "  \n",
    "    return df_all, (best_base_shift, best_fraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1) Ensure data_folder has train.csv + mix_*.csv\n",
    "# df_results, (best_bs, best_fr) = tune_mechinterp_hparams(\n",
    "#     folder=\"data\",\n",
    "#     base_shift_candidates=[0.05, 0.1, 0.2],\n",
    "#     fraction_candidates=[0.3, 0.5, 0.7],\n",
    "#     n_rounds=3,\n",
    "#     final_train_size=None  # or a specific integer\n",
    "# )\n",
    "# print(\"\\nFull Results:\\n\", df_results.head(20))\n",
    "# print(f\"\\nBest MechInterp Hyperparameters found: base_shift_factor={best_bs}, fraction_to_shift={best_fr}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
