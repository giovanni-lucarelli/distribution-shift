{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Huge \\blue{\\textbf{Simple Covariate Shift \\qquad}} \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    classification_report, roc_auc_score, accuracy_score, \n",
    "    f1_score, roc_curve\n",
    ")\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from pygam import s, te, f, LogisticGAM\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import ortho_group\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from src.data_generation import *\n",
    "from src.analysis import ModelEvaluator\n",
    "from src.utils import *\n",
    "from src.plotting import visualize_feature_shifts\n",
    "from src.grid_search import *\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "GRID_SEARCH = False\n",
    "PLOT = False\n",
    "GEN_DATA = False\n",
    "OVERFIT = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = 'data'\n",
    "\n",
    "# Parameter definition\n",
    "\n",
    "num_samples = 10000\n",
    "num_features = 3\n",
    "\n",
    "# degree of the polinomio for the attribute relationship\n",
    "degree = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GEN_DATA:\n",
    "    # random multivariate\n",
    "\n",
    "    mean_train = [0.90920214, 0.81962487, 0.88819135]\n",
    "\n",
    "    covariance_train = np.array([[0.726318, 0.20240102, 0.52472545],\n",
    "                                [0.20240102, 0.11392557, 0.0264108],\n",
    "                                [0.52472545, 0.0264108, 1.05107627]])\n",
    "\n",
    "    # build the features sample\n",
    "    sample_train = build_multivariate_sample(num_samples, mean_train, covariance_train)\n",
    "    sample_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if GEN_DATA:\n",
    "    df_train = pd.DataFrame(sample_train, columns=[f'X{i+1}' for i in range(num_features)])\n",
    "\n",
    "    # build target variable y\n",
    "    # random coefficients (otherwise remove coef from build_poly_target and will be randomly generated)\n",
    "    coef = [-0.8061577012389105, -0.3621987584904036, -0.16057091147074054, 0.4803476403769713, -0.10624889645240687, \n",
    "            0.3182084398201366, 0.6789895126695962, -0.791324832566177, 0.531479159887424, 0.49115959567000167]\n",
    "\n",
    "    y_train, z_train, coef_train = build_poly_target(sample_train, degree, coef)\n",
    "    df_train['Y'] = y_train\n",
    "    df_train['Z'] = z_train\n",
    "\n",
    "    # check for balance\n",
    "    df_train['Y'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GEN_DATA:\n",
    "    df_train = pd.DataFrame(sample_train, columns=[f'X{i+1}' for i in range(num_features)])\n",
    "\n",
    "    # build target variable y\n",
    "    # random coefficients (otherwise remove coef from build_poly_target and will be randomly generated)\n",
    "    coef = [-0.8061577012389105, -0.3621987584904036, -0.16057091147074054, 0.4803476403769713, -0.10624889645240687, \n",
    "            0.3182084398201366, 0.6789895126695962, -0.791324832566177, 0.531479159887424, 0.49115959567000167]\n",
    "\n",
    "    y_train, z_train, coef_train = build_poly_target(sample_train, degree, coef)\n",
    "    df_train['Y'] = y_train\n",
    "    df_train['Z'] = z_train\n",
    "\n",
    "    # check for balance\n",
    "    df_train['Y'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head() if GEN_DATA else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Sets: Shifted Distribution Mixtures\n",
    "\n",
    "To be as general as possible, we consider statistical mixtures and study the presumed progressive degradation in performance for increasingly pure mixtures towards the test distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GEN_DATA:\n",
    "    # shifted random multivariate\n",
    "    mean_shift = attributes_quantile(df_train, 0.05)\n",
    "\n",
    "    covariance_shift = [[ 0.16309729,  0.19325742, -0.12621892],\n",
    "                        [ 0.19325742,  0.25197638, -0.13972381],\n",
    "                        [-0.12621892, -0.13972381,  0.19160666]]\n",
    "\n",
    "    # Initialize an empty dictionary to store the dataframes\n",
    "    df_dict = {}\n",
    "\n",
    "    # Iterate over mix_prob values\n",
    "    for mix_prob in [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]:\n",
    "        # Generate mixture sample\n",
    "        sample_mix = build_mixture_sample(num_samples, mean_train, covariance_train, mean_shift, covariance_shift, mix_prob=mix_prob)\n",
    "\n",
    "        # Create a DataFrame for the features\n",
    "        df_mix = pd.DataFrame(sample_mix, columns=[f'X{i+1}' for i in range(num_features)])\n",
    "\n",
    "        # Build the target variable y\n",
    "        y_mix, z_mix, coef_mix = build_poly_target(sample_mix, degree, coefficients=coef_train)\n",
    "        df_mix['Y'] = y_mix\n",
    "        df_mix['Z'] = z_mix\n",
    "\n",
    "        # Store the DataFrame in the dictionary\n",
    "        df_dict[mix_prob] = df_mix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remark: the 0.0 is a sample from the distribution that generated the training set. Since `build_mixture_sample` function do the dample each time, the 0.0 sample can be used as test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Data to Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GEN_DATA:\n",
    "    # Create a folder\n",
    "    os.makedirs(DATA_FOLDER, exist_ok=True)\n",
    "\n",
    "    for mix_prob, df in df_dict.items():\n",
    "        df.to_csv(os.path.join(DATA_FOLDER, f'mix_{mix_prob}.csv'), index=False)\n",
    "    file_name = 'Parameters.txt'\n",
    "    file_path = os.path.join(DATA_FOLDER, file_name)\n",
    "    df_train.to_csv(os.path.join(DATA_FOLDER, 'train.csv'), index=False)\n",
    "\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write('Polinomial coefficients\\n')\n",
    "        f.write(f'{coef_train}\\n')\n",
    "        f.write('Mean train\\n')\n",
    "        f.write(f'{mean_train}\\n')\n",
    "        f.write('Covariance train\\n')\n",
    "        f.write(f'{covariance_train}\\n')\n",
    "        f.write('Mean shift\\n')\n",
    "        f.write(f'{mean_shift}\\n')\n",
    "        f.write('Covariance shift\\n')\n",
    "        f.write(f'{covariance_shift}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not GEN_DATA:\n",
    "    # Read data\n",
    "    df_train = pd.read_csv(os.path.join(DATA_FOLDER, 'train.csv'))\n",
    "    X_train = df_train.drop(columns=['Y', 'Z'])\n",
    "    y_train = df_train['Y']\n",
    "    z_train = df_train['Z']\n",
    "    df_dict = {}\n",
    "    for mix_prob in [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]:\n",
    "        df_dict[mix_prob] = pd.read_csv(os.path.join(DATA_FOLDER, f'mix_{mix_prob}.csv'))\n",
    "    \n",
    "    with open(os.path.join(DATA_FOLDER, \"Parameters.txt\"), 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        # Line with coefficients (second line)\n",
    "        coef_line = lines[1].strip()\n",
    "        # Convert string representation of list to actual list\n",
    "        coef_train = eval(coef_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head() if not GEN_DATA else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 2. Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polinomio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PLOT:\n",
    "    # Generate a grid of x1, x2, and x3 values\n",
    "    x1_arr = np.linspace(-3, 3, 20)  # Reduced the number of points to 20\n",
    "\n",
    "    x1, x2, x3 = np.meshgrid(x1_arr, x1_arr, x1_arr)\n",
    "\n",
    "    # Flatten the grid to pass into the polynomial function\n",
    "    x1_flat = x1.flatten()\n",
    "    x2_flat = x2.flatten()\n",
    "    x3_flat = x3.flatten()\n",
    "\n",
    "    # Create a combined array of x1, x2, and x3 values\n",
    "    samples = np.column_stack((x1_flat, x2_flat, x3_flat))\n",
    "\n",
    "    # Calculate the polynomial values for the grid\n",
    "    _, z_values, _ = build_poly_target(samples, degree, coefficients=coef_train)\n",
    "\n",
    "    # Reshape the z values back to the grid shape\n",
    "    z_values = z_values.reshape(x1.shape)\n",
    "\n",
    "    # Create 3D surface plots for all combinations\n",
    "    fig = plt.figure(figsize=(18, 12))\n",
    "\n",
    "    # x1, x2 vs z\n",
    "    ax1 = fig.add_subplot(231, projection='3d')\n",
    "    surf1 = ax1.plot_surface(x1[:, :, 0], x2[:, :, 0], z_values[:, :, 0], cmap=cm.viridis, edgecolor='none')\n",
    "    ax1.set_xlabel('X1')\n",
    "    ax1.set_ylabel('X2')\n",
    "    ax1.set_zlabel('Z')\n",
    "    ax1.set_title('X1, X2 vs Z')\n",
    "    fig.colorbar(surf1, ax=ax1, shrink=0.5, aspect=5)\n",
    "\n",
    "    # x1, x3 vs z\n",
    "    ax2 = fig.add_subplot(232, projection='3d')\n",
    "    surf2 = ax2.plot_surface(x1[0, :, :], x3[0, :, :], z_values[0, :, :], cmap=cm.viridis, edgecolor='none')\n",
    "    ax2.set_xlabel('X1')\n",
    "    ax2.set_ylabel('X3')\n",
    "    ax2.set_zlabel('Z')\n",
    "    ax2.set_title('X1, X3 vs Z')\n",
    "    fig.colorbar(surf2, ax=ax2, shrink=0.5, aspect=5)\n",
    "\n",
    "    # x2, x3 vs z\n",
    "    ax3 = fig.add_subplot(233, projection='3d')\n",
    "    surf3 = ax3.plot_surface(x2[:, 0, :], x3[:, 0, :], z_values[:, 0, :], cmap=cm.viridis, edgecolor='none')\n",
    "    ax3.set_xlabel('X2')\n",
    "    ax3.set_ylabel('X3')\n",
    "    ax3.set_zlabel('Z')\n",
    "    ax3.set_title('X2, X3 vs Z')\n",
    "    fig.colorbar(surf3, ax=ax3, shrink=0.5, aspect=5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "if PLOT:\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "    # Plot X1 vs Z\n",
    "    sns.scatterplot(ax=axes[0, 0], x=df_train['X1'], y=z_train)\n",
    "    axes[0, 0].set_title('X1 vs Z')\n",
    "    axes[0, 0].set_xlabel('X1')\n",
    "    axes[0, 0].set_ylabel('Z')\n",
    "\n",
    "    # Plot X2 vs Z\n",
    "    sns.scatterplot(ax=axes[0, 1], x=df_train['X2'], y=z_train)\n",
    "    axes[0, 1].set_title('X2 vs Z')\n",
    "    axes[0, 1].set_xlabel('X2')\n",
    "    axes[0, 1].set_ylabel('Z')\n",
    "\n",
    "    # Plot X3 vs Z\n",
    "    sns.scatterplot(ax=axes[0, 2], x=df_train['X3'], y=z_train)\n",
    "    axes[0, 2].set_title('X3 vs Z')\n",
    "    axes[0, 2].set_xlabel('X3')\n",
    "    axes[0, 2].set_ylabel('Z')\n",
    "\n",
    "    # Plot X1 vs Y\n",
    "    sns.scatterplot(ax=axes[1, 0], x=df_train['X1'], y=df_train['Y'])\n",
    "    axes[1, 0].set_title('X1 vs Y')\n",
    "    axes[1, 0].set_xlabel('X1')\n",
    "    axes[1, 0].set_ylabel('Y')\n",
    "\n",
    "    # Plot X2 vs Y\n",
    "    sns.scatterplot(ax=axes[1, 1], x=df_train['X2'], y=df_train['Y'])\n",
    "    axes[1, 1].set_title('X2 vs Y')\n",
    "    axes[1, 1].set_xlabel('X2')\n",
    "    axes[1, 1].set_ylabel('Y')\n",
    "\n",
    "    # Plot X3 vs Y\n",
    "    sns.scatterplot(ax=axes[1, 2], x=df_train['X3'], y=df_train['Y'])\n",
    "    axes[1, 2].set_title('X3 vs Y')\n",
    "    axes[1, 2].set_xlabel('X3')\n",
    "    axes[1, 2].set_ylabel('Y')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PLOT:\n",
    "    from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "    fig = plt.figure(figsize=(18, 12))\n",
    "\n",
    "    # Plot X1 vs X2 vs Z\n",
    "    ax = fig.add_subplot(231, projection='3d')\n",
    "    ax.scatter(df_train['X1'], df_train['X2'], z_train, c=z_train, cmap='viridis')\n",
    "    ax.set_title('X1 vs X2 vs Z')\n",
    "    ax.set_xlabel('X1')\n",
    "    ax.set_ylabel('X2')\n",
    "    ax.set_zlabel('Z')\n",
    "\n",
    "    # Plot X1 vs X3 vs Z\n",
    "    ax = fig.add_subplot(232, projection='3d')\n",
    "    ax.scatter(df_train['X1'], df_train['X3'], z_train, c=z_train, cmap='viridis')\n",
    "    ax.set_title('X1 vs X3 vs Z')\n",
    "    ax.set_xlabel('X1')\n",
    "    ax.set_ylabel('X3')\n",
    "    ax.set_zlabel('Z')\n",
    "\n",
    "    # Plot X2 vs X3 vs Z\n",
    "    ax = fig.add_subplot(233, projection='3d')\n",
    "    ax.scatter(df_train['X2'], df_train['X3'], z_train, c=z_train, cmap='viridis')\n",
    "    ax.set_title('X2 vs X3 vs Z')\n",
    "    ax.set_xlabel('X2')\n",
    "    ax.set_ylabel('X3')\n",
    "    ax.set_zlabel('Z')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " For higher dimensional data (n > 2), we can either:\n",
    "\n",
    " - Visualize a pairwise scatter matrix (e.g., `sns.pairplot`) for a subset of features.\n",
    "\n",
    " - Or just visualize a specified pair of features for a quick glimpse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PLOT:\n",
    "    # small_df_dict = {k: df_dict[k] for k in [0.0, 0.2, 0.5, 1.0]}\n",
    "    visualize_feature_shifts(df_dict=df_dict, features_to_plot= ['X1', 'X2', 'X3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PLOT:\n",
    "    sns.pairplot(df_train, vars=['X1', 'X2', 'X3'], hue='Y')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PLOT:\n",
    "    \n",
    "    sns.pairplot(df_dict[1.0], vars=['X1', 'X2', 'X3'], hue='Y')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Models Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train data\n",
    "\n",
    "X_train = df_train.drop(['Y','Z'], axis=1)\n",
    "y_train = df_train['Y']\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lreg_grid = {\n",
    "    'penalty': ['l1','l2'],\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'solver': ['liblinear']\n",
    "}\n",
    "\n",
    "\n",
    "if GRID_SEARCH:\n",
    "    lreg_model = grid_search_cv(LogisticRegression(), lreg_grid, X_train, y_train)\n",
    "else:\n",
    "    lreg_model = LogisticRegression(**best_params[\"LogisticRegression\"])\n",
    "    lreg_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgam_params = {\n",
    "    \"terms\"     : s(0) + s(1) + s(2) + te(0, 1) + te(0, 2) + te(1, 2),\n",
    "    \"max_iter\"  : 100\n",
    "}\n",
    "\n",
    "X_train_np = X_train.values  # Convert to NumPy array\n",
    "y_train_np = y_train.values  # Convert to NumPy array\n",
    "\n",
    "if GRID_SEARCH:\n",
    "    lgam_model = LogisticGAM(**lgam_params).gridsearch(X_train_np, y_train_np, lam=np.logspace(-3, 3, 10))\n",
    "else:\n",
    "    lgam_model = LogisticGAM(**lgam_params).fit(X_train_np, y_train_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgam_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid\n",
    "dtc_grid = {\n",
    "    'criterion'         : ['gini', 'entropy', 'log_loss'],\n",
    "    'max_depth'         : [3, 4, 5, 6],#, 7, 8], #9, 10, 11, 12],\n",
    "    'min_samples_leaf'  : [2, 4, 8, 16],\n",
    "    'splitter'          : ['best', 'random']\n",
    "}\n",
    "\n",
    "if GRID_SEARCH:\n",
    "    dtc_model = grid_search_cv(DecisionTreeClassifier(), dtc_grid, X_train, y_train)\n",
    "else:\n",
    "    dtc_model = DecisionTreeClassifier(**best_params[\"DecisionTreeClassifier\"])\n",
    "    dtc_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for Random Forest\n",
    "rf_grid = {\n",
    "    'n_estimators'      : [100, 125, 150],\n",
    "    'criterion'         : ['gini', 'entropy', 'log_loss'],\n",
    "    'max_depth'         : [3, 4, 5],\n",
    "    'min_samples_split' : [4, 5, 6],\n",
    "    'min_samples_leaf'  : [1, 4, 8],\n",
    "    'max_samples'       : [0.4, 0.5, 0.6],\n",
    "    'random_state'      : [0]\n",
    "}\n",
    "\n",
    "if GRID_SEARCH:\n",
    "    rfc_model = grid_search_cv(RandomForestClassifier(), rf_grid, X_train, y_train, cv = 10)\n",
    "else:\n",
    "    rfc_model = RandomForestClassifier(**best_params[\"RandomForestClassifier\"])\n",
    "    rfc_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for Gradient Boosting\n",
    "gbc_grid = {\n",
    "    'n_estimators'      : [100, 125, 150],\n",
    "    'learning_rate'     : [0.025, 0.01, 0.0075],\n",
    "    'max_depth'         : [3, 4, 5],\n",
    "    'min_samples_leaf'  : [6, 8, 10],\n",
    "    'subsample'         : [0.3, 0.4, 0.5, 0.6],\n",
    "    'max_features'      : [None]#, 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "if GRID_SEARCH:\n",
    "    gbc_model = grid_search_cv(GradientBoostingClassifier(), gbc_grid, X_train, y_train, cv = 10)\n",
    "else:\n",
    "    gbc_model = GradientBoostingClassifier(**best_params[\"GradientBoostingClassifier\"])\n",
    "    gbc_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extreme Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBoost Parameters\n",
    "xgb_grid = {\n",
    "    'n_estimators'      : [25, 50, 75],\n",
    "    'learning_rate'     : [0.1, 0.3, 0.5],\n",
    "    'max_depth'         : [2, 4, 6],\n",
    "    'min_child_weight'  : [1, 2, 3],\n",
    "    'subsample'         : [0.7, 0.8, 0.9],\n",
    "    'colsample_bytree'  : [0.8, 1.0],\n",
    "    'reg_alpha'         : [1], #[0, 1, 5],\n",
    "    'reg_lambda'        : [1], #[1, 5, 10],\n",
    "    'gamma'             : [5], #[0, 0.1, 0.2],\n",
    "}\n",
    "\n",
    "xgb_grid_2 = {\n",
    "    \n",
    "}\n",
    "\n",
    "if GRID_SEARCH:\n",
    "    xgb_model = grid_search_cv_xgb(xgb_grid, X_train, y_train, n_jobs=-1)\n",
    "else:\n",
    "    xgb_model = xgb.XGBClassifier(**best_params[\"XGBoost\"])\n",
    "    xgb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 4. Model Evaluation on Shifted Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models to evaluate\n",
    "\n",
    "models = {\n",
    "    \"DecisionTreeClassifier\"        : dtc_model,\n",
    "    \"GradientBoostingClassifier\"    : gbc_model,\n",
    "    \"RandomForestClassifier\"        : rfc_model,\n",
    "    \"XGBoost\"                       : xgb_model,\n",
    "    \"LogisticGAM\"                   : lgam_model,\n",
    "    \"LogisticRegression\"            : lreg_model\n",
    "    }\n",
    "\n",
    "# Assuming df_dict is a dictionary with keys from 0.1 to 1.0\n",
    "test_datasets = [(key, df.drop(['Y','Z'], axis=1), df['Y']) for key, df in df_dict.items() if 0.0 <= key <= 1.0]\n",
    "\n",
    "evaluator = ModelEvaluator(models, test_datasets)\n",
    "evaluator.evaluate_models(show_metrics=False) # = True)\n",
    "#evaluator.plot_roc_curves()\n",
    "#evaluator.plot_roc_curves_per_dataset()\n",
    "evaluator.plot_auc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OVERFIT:\n",
    "    lreg_grid = {\n",
    "    'penalty'       : [None, 'l1','l2'],\n",
    "        'solver'    : ['liblinear', 'saga'],#, 'newton-cg', 'lbfgs'],\n",
    "        'C'         : [100, 250, 500],\n",
    "        'max_iter'  : [100, 250, 500],\n",
    "        'fit_intercept': [True, False],\n",
    "    }\n",
    "    \n",
    "    lreg_model_of, lreg_params_of, lreg_score_of = overfit_models(X_train, y_train, LogisticRegression(), lreg_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OVERFIT:\n",
    "    dtc_grid = {\n",
    "        \"max_depth\"         : [5, 10, 25, 50, 100, 150, 200],\n",
    "        \"min_samples_leaf\"  : [4, 8, 16, 32, 64, 128],\n",
    "        \"criterion\"         : ['gini', 'entropy', 'log_loss'],\n",
    "        \"splitter\"          : ['random', 'best']\n",
    "    }\n",
    "\n",
    "    dtc_model_of, dtc_params_of, dtc_score_of = overfit_models(X_train, y_train, DecisionTreeClassifier(), dtc_grid)\n",
    "    #{'criterion': 'log_loss', 'max_depth': 200, 'min_samples_leaf': 4, 'splitter': 'best'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OVERFIT:\n",
    "    rfc_grid = {\n",
    "        'criterion'         : ['gini', 'entropy'], #'log_loss'],\n",
    "        'n_estimators'      : [75, 100, 175],\n",
    "        'max_depth'         : [16, 32, 64],\n",
    "        'min_samples_split' : [2, 3, 4],\n",
    "        'min_samples_leaf'  : [1, 2, 4],\n",
    "        'random_state'      : [0]\n",
    "    }\n",
    "\n",
    "    rfc_model_of, rfc_params_of, rfc_score_of = overfit_models(X_train, y_train, RandomForestClassifier(), rfc_grid)\n",
    "    #{'criterion': 'gini', 'max_depth': 32, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 75, 'random_state': 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OVERFIT:\n",
    "    gbc_grid = {\n",
    "        'n_estimators'  : [100, 175, 200],\n",
    "        'learning_rate' : [0.05, 0.1],\n",
    "        'max_depth'     : [16, 32, 64],\n",
    "        'subsample'     : [1.0],\n",
    "        'max_features'  : [None]#, 'sqrt', 'log2']\n",
    "    }\n",
    "\n",
    "    gbc_model_of, gbc_params_of, gbc_score_of = overfit_models(X_train, y_train, GradientBoostingClassifier(), gbc_grid)\n",
    "    #{'learning_rate': 0.05, 'max_depth': 16, 'max_features': None, 'n_estimators': 175, 'subsample': 1.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OVERFIT:\n",
    "    xgb_grid = {\n",
    "        'learning_rate'     : [0.05, 0.075, 0.1],\n",
    "        'max_depth'         : [0, 16, 32], \n",
    "        #'subsample'         : [1.0],#[0.5, 0.7],\n",
    "        #'colsample_bytree'  : [0.5, 0.7],\n",
    "        'n_estimators'      : [250, 500, 700]\n",
    "    }\n",
    "\n",
    "    xgb_model_of, xgb_params_of, xgb_score_of = overfit_models(X_train, y_train, xgb.XGBClassifier(), xgb_grid)\n",
    "    #{'learning_rate': 0.05, 'max_depth': 32, 'n_estimators': 700, 'subsample': 1.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OVERFIT:\n",
    "    # Define the models to evaluate\n",
    "    models = {\n",
    "        \"DecisionTreeClassifier\"        : dtc_model_of,\n",
    "        \"RandomForestClassifier\"        : rfc_model_of, \n",
    "        \"GradientBoostingClassifier\"    : gbc_model_of,\n",
    "        \"XGBoost\"                       : xgb_model_of,\n",
    "        \"LogisticRegression\"            : lreg_model_of,\n",
    "        #\"LogisticGAM\"                  : lgam_model_of  #! still to be implemented\n",
    "    }\n",
    "\n",
    "    # Assuming df_dict is a dictionary with keys from 0.1 to 1.0\n",
    "    test_datasets = [(key, df.drop(['Y','Z'], axis=1), df['Y']) for key, df in df_dict.items() if 0.0 <= key <= 1.0]\n",
    "\n",
    "    evaluator = ModelEvaluator(models, test_datasets)\n",
    "    evaluator.evaluate_models(show_metrics=False) # = True)\n",
    "    #evaluator.plot_roc_curves()\n",
    "    #evaluator.plot_roc_curves_per_dataset()\n",
    "    evaluator.plot_auc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 5. Mechanistic-Interpretability-Guided Robust Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from typing import Optional, Tuple, Union\n",
    "#\n",
    "#from sklearn.base import BaseEstimator\n",
    "#from src.robust_training.mechanistic import MechanisticTrainer\n",
    "#try:\n",
    "#    from pygam import LogisticGAM\n",
    "#    PYGAM_AVAILABLE = True\n",
    "#except ImportError:\n",
    "#    PYGAM_AVAILABLE = False\n",
    "#\n",
    "#\n",
    "#def run_mechanistic_robust_training_and_eval(\n",
    "#    folder: str = \"dat\",\n",
    "#    target: str = 'Y',\n",
    "#    n_rounds: int = 2,\n",
    "#    model_type: str = 'gbc',  # Options: 'gbc', 'tree', 'gam'\n",
    "#    base_shift_factor: float = 100,\n",
    "#    fraction_to_shift: float = 0.9,\n",
    "#    final_train_size: Optional[int] = None,\n",
    "#    random_state: int = 42,\n",
    "#    noise_scale: float = 0.001,\n",
    "#    n_grad_steps: int = 1,\n",
    "#    top_k: int = 3,\n",
    "#    eps = 0.2, \n",
    "#    ball = False\n",
    "#) -> Tuple[BaseEstimator, BaseEstimator]:\n",
    "#    \"\"\"\n",
    "#    Trains both a baseline model and a robust model using MechanisticTrainer,\n",
    "#    then evaluates both models on all shifted test files in the specified folder.\n",
    "#\n",
    "#    Parameters\n",
    "#    ----------\n",
    "#    folder : str\n",
    "#        Directory containing 'train.csv' for training and 'mix_<n>.csv' for testing.\n",
    "#    target : str\n",
    "#        Name of the target variable in the datasets.\n",
    "#    n_rounds : int\n",
    "#        Number of augmentation rounds for MechanisticTrainer.\n",
    "#    model_type : str\n",
    "#        Type of model to use for robust training. Options: 'gbc', 'tree', 'gam'.\n",
    "#    base_shift_factor : float\n",
    "#        Magnitude by which to shift selected features during augmentation.\n",
    "#    fraction_to_shift : float\n",
    "#        Fraction of the dataset to select for augmentation each round.\n",
    "#    final_train_size : int or None\n",
    "#        If specified, downsample the final training set to this size.\n",
    "#    random_state : int\n",
    "#        Seed for reproducibility.\n",
    "#    noise_scale : float\n",
    "#        Standard deviation of Gaussian noise added to augmented samples.\n",
    "#    n_grad_steps : int\n",
    "#        Number of gradient-based steps per sample during augmentation.\n",
    "#    top_k : int\n",
    "#        Number of top features (by gradient magnitude) to shift per sample.\n",
    "#\n",
    "#    Returns\n",
    "#    -------\n",
    "#    baseline_model : BaseEstimator\n",
    "#        The baseline model trained on the original data.\n",
    "#    robust_model : BaseEstimator\n",
    "#        The robustly trained model using MechanisticTrainer.\n",
    "#    \"\"\"\n",
    "#\n",
    "#    # ----------------------------------------------------------------------\n",
    "#    # 1) Load original training data from \"mix_0.0.csv\"\n",
    "#    # ----------------------------------------------------------------------\n",
    "#    train_file = os.path.join(folder, \"train.csv\")\n",
    "#    if not os.path.exists(train_file):\n",
    "#        raise FileNotFoundError(f\"Training file '{train_file}' not found in folder '{folder}'.\")\n",
    "#\n",
    "#    df_orig = pd.read_csv(train_file)\n",
    "#    if target not in df_orig.columns:\n",
    "#        raise ValueError(f\"Target column '{target}' not found in '{train_file}'.\")\n",
    "#\n",
    "#    X_train = df_orig.drop(columns=[target])\n",
    "#    y_train = df_orig[target]\n",
    "#\n",
    "#    print(f\"Loaded training data from '{train_file}' with shape = {X_train.shape}\")\n",
    "#\n",
    "#    # ----------------------------------------------------------------------\n",
    "#    # 2) Train Baseline Model\n",
    "#    #    Ensure baseline uses the same model_type for fair comparison.\n",
    "#    # ----------------------------------------------------------------------\n",
    "#    print(\"\\n=== Training Baseline Model ===\")\n",
    "#    if model_type == 'gbc':\n",
    "#        baseline_model = GradientBoostingClassifier(\n",
    "#            n_estimators=100,\n",
    "#            learning_rate=0.05,\n",
    "#            max_depth=4,                        # default 3 ma fa pena, a 10 ha migliore AUC ma comunque minore f1, a 4/5 è fair\n",
    "#            random_state=random_state\n",
    "#        )\n",
    "#    elif model_type == 'tree':\n",
    "#        baseline_model = DecisionTreeClassifier(\n",
    "#            max_depth=10,                       # default 5 ma fa pena, a 10 ha peggiore AUC e comunque minore f1 ma è fair\n",
    "#            random_state=random_state\n",
    "#        )\n",
    "#    elif model_type == 'gam':\n",
    "#        if not PYGAM_AVAILABLE:\n",
    "#            raise ImportError(\"pyGAM is not installed. Install it via `pip install pygam` or choose another model type.\")\n",
    "#        baseline_model = LogisticGAM( verbose=False)\n",
    "#        \n",
    "#    elif model_type == 'rfc':\n",
    "#        baseline_model = RandomForestClassifier(\n",
    "#            n_estimators=100,\n",
    "#            max_depth=10,                   # default 5 ma fa pena, a 10 ha peggiore AUC e comunque minore f1 ma è fair\n",
    "#            random_state=random_state\n",
    "#        )\n",
    "#    \n",
    "#    else:\n",
    "#        raise ValueError(f\"Unsupported model_type '{model_type}'. Choose from ['gbc', 'tree', 'gam'].\")\n",
    "#\n",
    "#    baseline_model.fit(X_train, y_train)\n",
    "#    \n",
    "#    print(\"Baseline model trained.\")\n",
    "#    # Check class distribution in shifted data\n",
    "#    df_shifted = pd.read_csv('data/mix_1.0.csv')\n",
    "#    print(\"Class distribution in shifted data:\")\n",
    "#    print(df_shifted['Y'].value_counts(normalize=True))\n",
    "#\n",
    "#    # Check model predictions\n",
    "#    y_pred = baseline_model.predict(df_shifted.drop('Y', axis=1))\n",
    "#    print(\"\\nPrediction distribution:\")\n",
    "#    print(pd.Series(y_pred).value_counts(normalize=True))\n",
    "#    \n",
    "#\n",
    "#    # ----------------------------------------------------------------------\n",
    "#    # 3) Train Mechanistic-Interpretability-Guided Robust Model\n",
    "#    # ----------------------------------------------------------------------\n",
    "#    print(\"\\n=== Training Mechanistic-Interpretability-Guided Robust Model ===\")\n",
    "#    trainer = MechanisticTrainer(\n",
    "#        model_type=model_type,         # 'gbc', 'tree', 'gam'\n",
    "#        base_shift_factor=base_shift_factor,\n",
    "#        n_rounds=n_rounds,\n",
    "#        subset_size_fraction=fraction_to_shift,\n",
    "#        n_grad_steps=n_grad_steps,\n",
    "#        top_k=top_k,\n",
    "#        random_state=random_state,\n",
    "#        noise_scale=noise_scale,\n",
    "#       \n",
    "#        val_fraction=0.1,               # Fraction for validation split\n",
    "#        eps = eps,\n",
    "#        ball = ball\n",
    "#    )\n",
    "#\n",
    "#\n",
    "#\n",
    "#    # Fit the robust model\n",
    "#    trainer.fit(X_train, y_train)\n",
    "#    robust_model = trainer.model\n",
    "#    print(\"Robust model trained.\")\n",
    "#\n",
    "#    # If final_train_size is specified, downsample & refit\n",
    "#    if final_train_size is not None and final_train_size < len(trainer.X_final):\n",
    "#        rng = np.random.RandomState(random_state)\n",
    "#        idx_down = rng.choice(len(trainer.X_final), size=final_train_size, replace=False)\n",
    "#        X_down = trainer.X_final.iloc[idx_down].reset_index(drop=True)\n",
    "#        y_down = trainer.y_final.iloc[idx_down].reset_index(drop=True)\n",
    "#        robust_model.fit(X_down, y_down)\n",
    "#        print(f\"Final training set downsampled to {final_train_size} samples.\")\n",
    "#        # Check class distribution in shifted data\n",
    "#    df_shifted = pd.read_csv('data/mix_1.0.csv')\n",
    "#    print(\"Class distribution in shifted data:\")\n",
    "#    print(df_shifted['Y'].value_counts(normalize=True))\n",
    "#\n",
    "#    # Check model predictions\n",
    "#    y_pred = robust_model.predict(df_shifted.drop('Y', axis=1))\n",
    "#    print(\"\\nPrediction distribution:\")\n",
    "#    print(pd.Series(y_pred).value_counts(normalize=True))\n",
    "#\n",
    "#    # ----------------------------------------------------------------------\n",
    "#    # 4) Evaluate on all shifted test files: \"mix_<n>.csv\"\n",
    "#    # ----------------------------------------------------------------------\n",
    "#    test_files = [\n",
    "#        f for f in os.listdir(folder)\n",
    "#        if f.startswith(\"mix_\") and f.endswith(\".csv\") \n",
    "#    ]\n",
    "#\n",
    "#    if not test_files:\n",
    "#        print(f\"\\nNo shifted test files found in '{folder}' for evaluation.\")\n",
    "#        return baseline_model, robust_model\n",
    "#\n",
    "#    print(\"\\n=== Evaluation on Shifted Test Files ===\")\n",
    "#    for test_file in sorted(test_files):\n",
    "#        test_path = os.path.join(folder, test_file)\n",
    "#        df_test = pd.read_csv(test_path)\n",
    "#        if target not in df_test.columns:\n",
    "#            print(f\"Skipping '{test_file}': missing target '{target}'.\")\n",
    "#            continue\n",
    "#\n",
    "#        X_test = df_test.drop(columns=[target])\n",
    "#        y_test = df_test[target]\n",
    "#\n",
    "#        # Evaluate Baseline Model\n",
    "#        y_pred_b = baseline_model.predict(X_test)\n",
    "#        if hasattr(baseline_model, \"predict_proba\"):\n",
    "#            y_proba_b = baseline_model.predict_proba(X_test)[:, 1]\n",
    "#            try:\n",
    "#                auc_b = roc_auc_score(y_test, y_proba_b)\n",
    "#            except ValueError:\n",
    "#                auc_b = \"N/A (only one class present)\"\n",
    "#        else:\n",
    "#            y_proba_b = None\n",
    "#            auc_b = \"N/A\"\n",
    "#\n",
    "#        acc_b = accuracy_score(y_test, y_pred_b)\n",
    "#        f1_b = f1_score(y_test, y_pred_b) \n",
    "#\n",
    "#        # Evaluate Robust Model\n",
    "#        y_pred_r = robust_model.predict(X_test)\n",
    "#        if hasattr(robust_model, \"predict_proba\"):\n",
    "#            y_proba_r = robust_model.predict_proba(X_test)[:, 1]\n",
    "#            try:\n",
    "#                auc_r = roc_auc_score(y_test, y_proba_r)\n",
    "#            except ValueError:\n",
    "#                auc_r = \"N/A (only one class present)\"\n",
    "#        else:\n",
    "#            y_proba_r = None\n",
    "#            auc_r = \"N/A\"\n",
    "#\n",
    "#        acc_r = accuracy_score(y_test, y_pred_r)\n",
    "#        f1_r = f1_score(y_test, y_pred_r) \n",
    "#        print(f\"\\nTest File: {test_file}\")\n",
    "#        print(f\"  Baseline Model => Accuracy: {acc_b:.3f}, F1 Score: {f1_b:.3f}, AUC: {auc_b}\")\n",
    "#        print(f\"  Robust Model   => Accuracy: {acc_r:.3f}, F1 Score: {f1_r:.3f}, AUC: {auc_r}\")\n",
    "#        print(\"-\" * 50)\n",
    "#\n",
    "#        print(f\"\\nTest File: {test_file}: delta AUC r-b (hope > 0): {auc_r - auc_b}\")\n",
    "#\n",
    "#    return baseline_model, robust_model\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_FOLDER = 'data'\n",
    "# TARGET_COLUMN = 'Y'\n",
    "\n",
    "# # Run the robust training and evaluation\n",
    "# baseline_model, robust_model = run_mechanistic_robust_training_and_eval(\n",
    "#     folder=DATA_FOLDER,\n",
    "#     target=TARGET_COLUMN,\n",
    "#     n_rounds=1,\n",
    "#     model_type='rfc',          # Options: 'gbc', 'tree', 'gam', 'rfc'\n",
    "#     base_shift_factor=3.5,    \n",
    "#     fraction_to_shift=0.9,\n",
    "#     final_train_size=None,     # Keep the original size\n",
    "#     random_state=42,\n",
    "#     noise_scale=0.01,\n",
    "#     n_grad_steps=1,\n",
    "#     top_k=3, \n",
    "#     eps=0.2,\n",
    "#     ball=True\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_FOLDER = 'data'\n",
    "# TARGET_COLUMN = 'Y'\n",
    "\n",
    "# for eps in np.arange(0.1, 1.0, 0.1):\n",
    "#     print(f\"###############################Running for epsilon = {eps}\")\n",
    "#     # Run the robust training and evaluation\n",
    "#     baseline_model, robust_model = run_mechanistic_robust_training_and_eval(\n",
    "#         folder=DATA_FOLDER,\n",
    "#         target=TARGET_COLUMN,\n",
    "#         n_rounds=1,\n",
    "#         model_type='rfc',          # Options: 'gbc', 'tree', 'gam', 'rfc'\n",
    "#         base_shift_factor=100,    \n",
    "#         fraction_to_shift=0.9,\n",
    "#         final_train_size=1000,     # Keep the original size\n",
    "#         random_state=42,\n",
    "#         noise_scale=0.0001,\n",
    "#         n_grad_steps=5,\n",
    "#         top_k=3, \n",
    "#         eps=eps\n",
    "#     )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
