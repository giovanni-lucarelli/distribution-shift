\chapter{Performance Degradation}

\section{Evaluation Metric: AUC}

We used the \textbf{Area under the Curve} (\textbf{AUC}) to evaluate model performance. The AUC measures the performance of a classification model by calculating the area under the ROC curve, which plots the true positive rate against the false positive rate. The AUC ranges from 0 to 1, where 0 indicates no predictive power and 1 indicates perfect predictive power.

Because the AUC focuses on how well the model ranks positive instances above negative ones, it is generally robust to shifts in the data distribution: if a positive and a negative example are chosen at random, the model should consistently rank the positive example higher than the negative one. 

\section{Models}

As metioned in the first chapter, simple covariate shift can lead to serious degradations in model performance. In this chapter, we will analyze how different statistical learning models' performances are affected by covariate shift. 

We evaluated a diverse set of models, ranging from simple linear classifiers to more sophisticated ensemble methods, to comprehensively assess their robustness under covariate shift conditions:

\begin{itemize}
    \item \textbf{Logistic Regression}: A linear classifier that serves as a baseline model;
    \item \textbf{Decision Tree}: A non-linear model that creates a tree-like structure of decision rules;
    \item \textbf{Generalized Additive Model (GAM)}: A flexible statistical model that combines the interpretability of linear models with the ability to capture non-linear relationships;
    \item \textbf{Gradient Boosted Trees}: An ensemble learning method that creates sequential decision trees to iteratively correct prediction errors. Each tree focuses on the residuals from previous predictions, with optimized step sizes to minimize the overall loss function using gradient descent;
    \item \textbf{eXtreme Gradient Boosting (XGBoost)}: A highly optimized implementation of gradient boosting machines known for its efficiency and performance.
\end{itemize}

Each model was selected for its unique characteristics and widespread use in practical applications, providing a comprehensive view of how different learning approaches handle distribution shifts.

\section{Results}

We firstly evaluated the performance of each vanilla model on the aforementioned statistical mixtures of original and shifted test data. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{assets/vanilla.png} 
    \caption{\textbf{Performance comparison of vanilla models under covariate shift.} The AUC scores of the models are plotted against the mixing probability $p$, which represents the proportion of shifted data in the test set.}
    \label{fig:vanilla-models-perf}
\end{figure}

As we can see from the plot, at low levels of mixed data, the models can still perform realatively well. However, as the proportion of mixed data increases, the performance of the models degrades significantly. The decision tree model is the most sensitive to covariate shift, while the GAM model is the most robust.

Given these preliminary results, we proceeded with hyperparameter optimization to potentially enhance model performance.

\subsubsection{Fine Tuning}

The best choice for hyperparameters in these models might be different depending on the dataset. In order to find the hyperparameters that best fit the data, we performed a hyperparameter tuning using the \plaintt{GridSearchCV} function from the \plaintt{scikit-learn} library, which performs a cross-validation (k=5) to select the best hyperparameters for each model.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{assets/tuned.png} 
    \caption{\textbf{Performance comparison of fine tuned models under covariate shift.}}
    \label{fig:tuned-models-perf}
\end{figure}

As shown in \cref{fig:tuned-models-perf}, the fine-tuned models exhibit improved performance compared to the vanilla models. The decision tree model, which was the most sensitive to covariate shift, now performs almost on par with the other models. The GAM model remains the most robust, with consistently high AUC scores across all mixing probabilities. Bear in mind that we are still working on the fine tuning.
