\chapter{Conclusions}

In this work we explored how covariate shift leads to performance degradation in the roc-auc performance in a variety of models. We defined an original procedure in order to generate syntetic datasets with peculiar properties, \textit{i.e.} multinormality of the features, 2nd order polinomial relationship with random coefficients. This properties togheter with random covariance matrices generation, allowed us to have a controlled and continuos way to simulate covariate shifts. In this framework, we trained several ensemble models and evaluated their performances against a logistic classifier baseline. We found ways to efficiently parallelize the workload of tuning the ensemble models and generating data. Nontheless what the analysis shows is that the degree of degradation is equal inn the more complex models as they all perform similiarly, while the logistic classifier served as a good baseline. Interestingly, in some instances of the problem, the latter performed as well as the ensemble models. We think this result is due to the logistic classifier's higher variance. We already answer on the proposed question of finding the best model, among the proposed ones, to faces distribution shift, they do not differ. What about \textit{why} every model beheaves in a similiar fashion? Our hypothesis is that the dataset was never complex enough to really challenge the models -- we had to limit the features number to 3 because of compute constraints, moreover the quadratic relationship was not enough to make the problem hard. So, for this particular problem definition, all the models are able to achive an {upperbound} that is intrinsic to the problem itself. 

Our original goal was to develop an algorithm that, differing from the literature we read, didn't have any prior knowledge on the shift, \textit{i.e.} on the testing datasets. At the core of this problem lays the fact that no data augmentation can be relevant enough when its not reflecting the nature of the real shift. The R.A.W. algorithm quickly became yet another way to assess the difficulty of performing well in a covariate shift scenario. Unfortunately, it didn't led to significant results but we still believe that not relying on any estimation on the testing set is a novel and still to be walked path.

We are not proposing any future work, we have many questions that we would like to answer: What happens if we use Real World datasets? Is it possible to improve the R.A.W. algorithm? And if so, how?
